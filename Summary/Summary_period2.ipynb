{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summary_period2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#notebook to download the csv of edges and nodes of a given network\n",
        "import os\n",
        "import requests \n",
        "import time\n",
        "import string\n",
        "import networkx as nx\n",
        "import itertools\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.corpus import wordnet as wn #importing it\n",
        "from nltk.stem.wordnet import WordNetLemmatizer #importing wordnet lemmatizer\n",
        "from nltk import pos_tag #part-of-speech-tagger\n",
        "from collections import defaultdict #defaultdict returns default value for non-existant keys you try to  access based on the function you passed in the constructor\n",
        "from google.colab import files\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRcCylkZqDPi",
        "outputId": "1f276ac2-d067-4422-98b2-a24dac9c11b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(df):       #extract the text from the tweets and RT\n",
        "                            #works ONLY on .csv file\n",
        "  list_strings = []\n",
        "  for index in range(len(df)):\n",
        "    if index % 1000 == 0:\n",
        "      print(str(index)+' / '+str(len(df)))\n",
        "    text = df.loc[index]['text']                          #if it is nor trucated nor a RT  i take \"text\"\n",
        "    string = -1\n",
        "    if (df.loc[index,\"truncated\"] == True):                 #if it is trucated I take \"extended_tweet\"\n",
        "        string = df.loc[index,\"extended_tweet\"]\n",
        "    if type(df.loc[index,\"retweeted_status\"]) != float:     #if it is a RT I take retweeted_status\n",
        "        string = df.loc[index,\"retweeted_status\"]\n",
        "    if type(string) == str :\n",
        "        if(re.search('full_text\\':(.+?)https',string) != None):     #if I find \"full_text\"\n",
        "          s = re.search('full_text\\':(.+?)https',string).group(1)\n",
        "        if(re.search('text\\':(.+?)https',string)!= None):\n",
        "          s = re.search('text\\':(.+?)https',string).group(1)\n",
        "        else: \n",
        "          continue\n",
        "        list_strings.append(s)\n",
        "        #print(s)         \n",
        "    else:\n",
        "      list_strings.append(text)\n",
        "      #print(text)\n",
        "      \n",
        "\n",
        "  return list_strings"
      ],
      "metadata": {
        "id": "CGa_FVVGqDSJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning, lemmatising and pos tagging tweets\n",
        "\n",
        "nltk.download('words')\n",
        "WORDS = set(nltk.corpus.words.words()) #the last two lines serve to download the corpus of standard English language words\n",
        "nltk.download('stopwords') #downloading stopwords\n",
        "STOP_WORDS = set(nltk.corpus.stopwords.words(\"english\")) #taking the stop words from English language\n",
        "nltk.download('wordnet') #downloading wordnet\n",
        "nltk.download('averaged_perceptron_tagger') #downloading tagger\n",
        "tag_map = defaultdict(lambda : wn.NOUN) #here we define that wn.NOUN is the default value for the dict\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "def lemma_pos_cleaner(tweet):\n",
        "\n",
        "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) # remove mentions\n",
        "    tweet = re.sub(\"#[A-Za-z0-9]+\", \"\",tweet) # remove hashtags\n",
        "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) # remove http links\n",
        "    tweet = \" \".join(tweet.split())\n",
        "    tweet = str.lower(tweet) #to lowercase \n",
        "    tweet = re.sub(\"'\",\" \",tweet) # remove aphostrophe\n",
        "\n",
        "    #basically we use pos_tag function on tokens that we get by applying wordpunct tokenization\n",
        "    #to tweet (it separates all the words and symbols)\n",
        "    #then we pass the token along with it's wordnet pos value that we get from the tag_map dictionary (noun, adjective, verb or adverb) to the lemma function (the WordNetLemmatizer())\n",
        "    lemma_function = WordNetLemmatizer()\n",
        "    tweet = \" \".join(lemma_function.lemmatize(token, tag_map[tag[0]]) for token, tag in nltk.pos_tag(nltk.wordpunct_tokenize(tweet))) #lemmatize\n",
        "  \n",
        "\n",
        "    # francesco: I removed also all 2 letters words and added specific words, words that appears frequently but are discarded because they are not in the english language\n",
        "    SPECIFIC_WORDS = ['virus', 'coronavirus', 'covid19', 'covid', 'trump', 'hubei', 'beijing', 'xinjiang', 'jinping', 'korea', 'xinhua', 'india', 'taiwan','johnson','singapore', 'africa', 'japanese', 'france', 'asian', 'australia', 'french', 'asia', 'leishenshan', 'british', 'qingdao', 'fauci', 'america',  'california', 'sichuan', 'malaysia', 'huawei','thailand', 'shandong', 'italy', 'philippines', 'germany', 'facebook', 'african', 'shenzhen', 'tokyo', 'russian','uygur', '5g', 'pompeo', 'vietnam', 'australian', 'cambodia', 'zhejiang', 'yunnan', 'guangdong', 'korean', 'iran', 'washington']\n",
        "    tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) if (w in WORDS or w in SPECIFIC_WORDS) and len(w)>2 and w not in STOP_WORDS ) #remove stop words\n",
        "   \n",
        "    return tweet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD0LDoYVqDUh",
        "outputId": "8b6ff527-7e43-426f-c9f1-d78ad7b5bb26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def frequency_dictionary(df):\n",
        "  unique_words = {}\n",
        "\n",
        "  for row in df:\n",
        "    for word in row.split():\n",
        "      #if the word is encountered for the first time add to dict as key and set its value to 0\n",
        "      unique_words.setdefault(word,0)\n",
        "      #increase the value (i.e the count) of the word by 1 every time it is encountered\n",
        "      unique_words[word] += 1\n",
        "\n",
        "  return unique_words"
      ],
      "metadata": {
        "id": "MAfOr8vR4P88"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "period = '_SeptOct2020'  # _JanFeb2020, _MarchApril2021, _SeptOct2020"
      ],
      "metadata": {
        "id": "o-GUlzAVOwk_"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "China = pd.read_csv('/content/China'+period+'.csv')\n",
        "USA = pd.read_csv('/content/USA'+period+'.csv')\n",
        "China_USA = pd.read_csv('/content/China&USA'+period+'.csv')"
      ],
      "metadata": {
        "id": "suPdLaUx1Ct8"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of tweets:\n",
        "print('China: ', len(China))\n",
        "print('USA: ', len(USA))\n",
        "print('China&USA: ', len(China_USA))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeO9nxO6Br3x",
        "outputId": "a7534556-8e51-44e6-a2ca-cd2f28252749"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  3098\n",
            "USA:  8389\n",
            "China&USA:  11487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_China = extract_text(China)\n",
        "text_USA = extract_text(USA)\n",
        "text_China_USA = extract_text(China_USA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WI3Rz9T1Cxo",
        "outputId": "79ae9343-789d-4551-b87a-f51d560904bd"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 3098\n",
            "1000 / 3098\n",
            "2000 / 3098\n",
            "3000 / 3098\n",
            "0 / 8389\n",
            "1000 / 8389\n",
            "2000 / 8389\n",
            "3000 / 8389\n",
            "4000 / 8389\n",
            "5000 / 8389\n",
            "6000 / 8389\n",
            "7000 / 8389\n",
            "8000 / 8389\n",
            "0 / 11487\n",
            "1000 / 11487\n",
            "2000 / 11487\n",
            "3000 / 11487\n",
            "4000 / 11487\n",
            "5000 / 11487\n",
            "6000 / 11487\n",
            "7000 / 11487\n",
            "8000 / 11487\n",
            "9000 / 11487\n",
            "10000 / 11487\n",
            "11000 / 11487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text_China = [lemma_pos_cleaner(txt) for txt in text_China]\n",
        "cleaned_text_USA = [lemma_pos_cleaner(txt) for txt in text_USA]\n",
        "cleaned_text_China_USA = [lemma_pos_cleaner(txt) for txt in text_China_USA]\n",
        "\n",
        "print('China:')\n",
        "print(cleaned_text_China[0:10])\n",
        "print()\n",
        "print('USA:')\n",
        "print(cleaned_text_USA[0:10])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(cleaned_text_China_USA[0:10])"
      ],
      "metadata": {
        "id": "6q4JI_jV1C2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ead2a5-f017-4d7d-9ac0-258bf5992937"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "['people beijing china tour popular destination like forbidden city pose wed picture shop car enjoy visit fall weather', 'erect people volunteer martyr cemetery northeastern china begin', 'china international medical equipment fair become showcase state art medical', 'new united nation report show china experience positive growth third quarter despite shrink', 'detect cold chain food east shandong', 'five year review china commit green development', 'report china country see positive trade growth', 'revenue first three quarter', 'potential record breaker tall male reach meter seven foot three inch age junior high student china sichuan furniture clothes distinguished trait', 'china build hospital within day infrastructure development one key point address country high speed rail new airport greatly develop past five year']\n",
            "\n",
            "USA:\n",
            "['india china country claim part region history', 'china ask india pledge return apprehend soldier', 'australia join india japan united state large scale military exercise coast india next month due concern rise influence', 'sanction tariff outright ban look price business china', 'would fight everybody anybody time different fight fight feel worthy time fight people china mac rapper', 'say canada cow china human right', 'hurt know country born raise hat see equal country fight right thing rapper', 'china reject news report say child separate parent', 'response china set pass new tech law would restrict sensitive export vital national security', 'china say peace ship sail strait']\n",
            "\n",
            "China&USA:\n",
            "['people beijing china tour popular destination like forbidden city pose wed picture shop car enjoy visit fall weather', 'erect people volunteer martyr cemetery northeastern china begin', 'china international medical equipment fair become showcase state art medical', 'new united nation report show china experience positive growth third quarter despite shrink', 'detect cold chain food east shandong', 'five year review china commit green development', 'report china country see positive trade growth', 'revenue first three quarter', 'potential record breaker tall male reach meter seven foot three inch age junior high student china sichuan furniture clothes distinguished trait', 'china build hospital within day infrastructure development one key point address country high speed rail new airport greatly develop past five year']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCiw0q7t4UER",
        "outputId": "478f589c-8e5c-4a4b-ddfa-0ab2e41086a3"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  4216\n",
            "USA:  4967\n",
            "China&USA:  6382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most frequent words\n",
        "print('China')\n",
        "print([key for key in freq_dict_China.keys() if freq_dict_China[key]>200])\n",
        "print()\n",
        "print('USA')\n",
        "print([key for key in freq_dict_USA.keys() if freq_dict_USA[key]>400])\n",
        "print()\n",
        "print('China&USA')\n",
        "print([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]>600])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y29OU9Cd5Auv",
        "outputId": "04e78b55-3740-4ad1-e6ee-1e2968a88806"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "['china', 'covid', 'say', 'case', 'new', 'coronavirus', 'country', 'year', 'day', 'president', 'report', 'city', 'national', 'people', 'world']\n",
            "\n",
            "USA\n",
            "['covid', 'coronavirus', 'say', 'trump', 'new', 'vaccine', 'china', 'case', 'president', 'test', 'pandemic', 'report', 'positive', 'house', 'day', 'people', 'state']\n",
            "\n",
            "China&USA\n",
            "['covid', 'china', 'coronavirus', 'say', 'new', 'trump', 'case', 'vaccine', 'president', 'test', 'report', 'pandemic', 'day', 'positive', 'people']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# less frequent words"
      ],
      "metadata": {
        "id": "9oHPXYbpINWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# less frequent words:\n",
        "print('Less frequent China: ', len([key for key in freq_dict_China.keys() if freq_dict_China[key]<5]))\n",
        "print('More frequent China: ', len([key for key in freq_dict_China.keys() if freq_dict_China[key]>=5]))\n",
        "less_frequent_words_China = [key for key in freq_dict_China.keys() if freq_dict_China[key]<5]\n",
        "\n",
        "print('Less frequent USA: ', len([key for key in freq_dict_USA.keys() if freq_dict_USA[key]<5]))\n",
        "print('More frequent USA:', len([key for key in freq_dict_USA.keys() if freq_dict_USA[key]>=5]))\n",
        "less_frequent_words_USA = [key for key in freq_dict_USA.keys() if freq_dict_USA[key]<5]\n",
        "\n",
        "print('Less frequent China&USA: ', len([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]<5]))\n",
        "print('More frequent China&USA: ', len([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]>=5]))\n",
        "less_frequent_words_China_USA = [key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]<5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgiQzaHD5zAb",
        "outputId": "54d820fa-a310-479b-a126-71a305100c28"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Less frequent China:  2825\n",
            "More frequent China:  1391\n",
            "Less frequent USA:  2759\n",
            "More frequent USA: 2208\n",
            "Less frequent China&USA:  3571\n",
            "More frequent China&USA:  2811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# discard less frequent words\n",
        "cleaned_mostfreq_text_China = cleaned_text_China.copy()\n",
        "for txt in range(len(cleaned_mostfreq_text_China)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostfreq_text_China))\n",
        "  for word in less_frequent_words_China:\n",
        "    if word in cleaned_mostfreq_text_China[txt].split():\n",
        "      cleaned_mostfreq_text_China[txt] = cleaned_mostfreq_text_China[txt].replace(word, '')\n",
        "      cleaned_mostfreq_text_China[txt] = \" \".join(cleaned_mostfreq_text_China[txt].split())\n",
        "\n",
        "cleaned_mostfreq_text_USA = cleaned_text_USA.copy()\n",
        "for txt in range(len(cleaned_mostfreq_text_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostfreq_text_USA))\n",
        "  for word in less_frequent_words_USA:\n",
        "    if word in cleaned_mostfreq_text_USA[txt].split():\n",
        "      cleaned_mostfreq_text_USA[txt] = cleaned_mostfreq_text_USA[txt].replace(word, '')\n",
        "      cleaned_mostfreq_text_USA[txt] = \" \".join(cleaned_mostfreq_text_USA[txt].split())\n",
        "\n",
        "cleaned_mostfreq_text_China_USA = cleaned_text_China_USA.copy()\n",
        "for txt in range(len(cleaned_mostfreq_text_China_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostfreq_text_China_USA))\n",
        "  for word in less_frequent_words_China_USA:\n",
        "    if word in cleaned_mostfreq_text_China_USA[txt].split():\n",
        "      cleaned_mostfreq_text_China_USA[txt] = cleaned_mostfreq_text_China_USA[txt].replace(word, '')\n",
        "      cleaned_mostfreq_text_China_USA[txt] = \" \".join(cleaned_mostfreq_text_China_USA[txt].split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmlqtzHp6lh8",
        "outputId": "97a0699e-e63f-46d9-93c9-4cebb1a5de45"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 3089\n",
            "1000 / 3089\n",
            "2000 / 3089\n",
            "3000 / 3089\n",
            "0 / 8384\n",
            "1000 / 8384\n",
            "2000 / 8384\n",
            "3000 / 8384\n",
            "4000 / 8384\n",
            "5000 / 8384\n",
            "6000 / 8384\n",
            "7000 / 8384\n",
            "8000 / 8384\n",
            "0 / 11473\n",
            "1000 / 11473\n",
            "2000 / 11473\n",
            "3000 / 11473\n",
            "4000 / 11473\n",
            "5000 / 11473\n",
            "6000 / 11473\n",
            "7000 / 11473\n",
            "8000 / 11473\n",
            "9000 / 11473\n",
            "10000 / 11473\n",
            "11000 / 11473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_mostfreq_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_mostfreq_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_mostfreq_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KHj5-o57f8y",
        "outputId": "1d8df488-6a7c-49f4-fdf8-f8d036451631"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  1408\n",
            "USA:  2222\n",
            "China&USA:  2828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Network"
      ],
      "metadata": {
        "id": "1davLfA3HNV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_China = pd.DataFrame.from_dict(freq_dict_China, orient='index').reset_index()\n",
        "df_China.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China.reset_index(inplace=True)\n",
        "df_China.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_USA = pd.DataFrame.from_dict(freq_dict_USA, orient='index').reset_index()\n",
        "df_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_USA.reset_index(inplace=True)\n",
        "df_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_China_USA = pd.DataFrame.from_dict(freq_dict_China_USA, orient='index').reset_index()\n",
        "df_China_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China_USA.reset_index(inplace=True)\n",
        "df_China_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "print('China')\n",
        "print(df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA')\n",
        "print(df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA')\n",
        "print(df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxPPnevj890b",
        "outputId": "9200c291-f25e-4d3e-c87d-c4067e41efd7"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "           Word  Count\n",
            "0         china   1751\n",
            "1         covid    641\n",
            "2           say    446\n",
            "3          case    324\n",
            "4           new    321\n",
            "5   coronavirus    272\n",
            "6       country    257\n",
            "7          year    256\n",
            "8           day    256\n",
            "9     president    238\n",
            "10       report    231\n",
            "11         city    227\n",
            "12     national    216\n",
            "13       people    211\n",
            "14        world    204\n",
            "15         test    193\n",
            "16     province    192\n",
            "17     pandemic    189\n",
            "18       health    171\n",
            "19        first    167\n",
            "20      million    167\n",
            "21        watch    154\n",
            "22       global    153\n",
            "23      vaccine    148\n",
            "24         live    134\n",
            "25        trump    129\n",
            "26         take    127\n",
            "27          see    124\n",
            "28     positive    122\n",
            "29         high    121\n",
            "\n",
            "USA\n",
            "           Word  Count\n",
            "0         covid   4188\n",
            "1   coronavirus   2205\n",
            "2           say   1662\n",
            "3         trump   1315\n",
            "4           new   1170\n",
            "5       vaccine   1168\n",
            "6         china   1073\n",
            "7          case   1055\n",
            "8     president   1017\n",
            "9          test   1006\n",
            "10     pandemic    602\n",
            "11       report    601\n",
            "12     positive    565\n",
            "13        house    461\n",
            "14          day    436\n",
            "15       people    434\n",
            "16        state    404\n",
            "17        death    395\n",
            "18       health    391\n",
            "19        first    387\n",
            "20      million    364\n",
            "21        white    361\n",
            "22        world    349\n",
            "23    infection    340\n",
            "24        month    332\n",
            "25        trial    332\n",
            "26         week    327\n",
            "27      country    320\n",
            "28         rise    318\n",
            "29          one    315\n",
            "\n",
            "China&USA\n",
            "           Word  Count\n",
            "0         covid   4829\n",
            "1         china   2824\n",
            "2   coronavirus   2477\n",
            "3           say   2108\n",
            "4           new   1491\n",
            "5         trump   1444\n",
            "6          case   1379\n",
            "7       vaccine   1316\n",
            "8     president   1255\n",
            "9          test   1199\n",
            "10       report    832\n",
            "11     pandemic    791\n",
            "12          day    692\n",
            "13     positive    689\n",
            "14       people    645\n",
            "15      country    577\n",
            "16       health    562\n",
            "17        first    554\n",
            "18        world    553\n",
            "19      million    531\n",
            "20         year    527\n",
            "21        house    526\n",
            "22        state    491\n",
            "23        death    488\n",
            "24          one    429\n",
            "25         city    426\n",
            "26        white    418\n",
            "27    infection    415\n",
            "28       global    402\n",
            "29         week    400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_China = freq_dict_China.keys()  \n",
        "keys_USA = freq_dict_USA.keys()  \n",
        "keys_China_USA = freq_dict_China_USA.keys()  "
      ],
      "metadata": {
        "id": "zHmEpbLtr46D"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_network(cleaned_text):\n",
        "  network = {}\n",
        "  #connect the word that appear in the same tweets\n",
        "  for row in cleaned_text:\n",
        "    combined_list = [word for word in str.split(row)]\n",
        "    #for pair in itertools.product(combined_list, combined_list):\n",
        "    #print(combined_list)\n",
        "    for pair in itertools.product(combined_list, combined_list):\n",
        "          #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
        "          if pair[0]!=pair[1] and not(pair[::-1] in network):\n",
        "              network.setdefault(pair,0)\n",
        "              network[pair] += 1 \n",
        "  network_df = pd.DataFrame.from_dict(network, orient=\"index\")\n",
        "  network_df.columns = [\"weight\"]\n",
        "  network_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
        "  return network, network_df"
      ],
      "metadata": {
        "id": "nUAWB6lT1DAd"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_China, network_df_China = create_network(cleaned_mostfreq_text_China)\n",
        "network_USA, network_df_USA = create_network(cleaned_mostfreq_text_USA)\n",
        "network_China_USA, network_df_China_USA = create_network(cleaned_mostfreq_text_China_USA)"
      ],
      "metadata": {
        "id": "7vR_r37vAGye"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(network_df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(network_df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(network_df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxMWDZua_b-l",
        "outputId": "9c4a2eca-d075-4f1b-841b-93f8a04e92dc"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "                      weight\n",
            "(china, say)             283\n",
            "(year, china)            218\n",
            "(china, covid)           212\n",
            "(china, day)             211\n",
            "(china, national)        185\n",
            "(report, case)           183\n",
            "(new, case)              181\n",
            "(national, day)          170\n",
            "(china, country)         152\n",
            "(covid, case)            152\n",
            "(china, province)        151\n",
            "(china, city)            149\n",
            "(poverty, china)         127\n",
            "(covid, say)             125\n",
            "(new, china)             125\n",
            "(new, report)            124\n",
            "(people, china)          121\n",
            "(day, holiday)           120\n",
            "(china, world)           120\n",
            "(test, positive)         117\n",
            "(new, covid)             117\n",
            "(china, xinjiang)        111\n",
            "(president, china)       111\n",
            "(china, holiday)         108\n",
            "(covid, pandemic)        108\n",
            "(china, development)     108\n",
            "(first, china)           105\n",
            "(global, china)          104\n",
            "(say, president)         101\n",
            "(health, case)            97\n",
            "\n",
            "USA:\n",
            "                          weight\n",
            "(president, trump)          1037\n",
            "(say, covid)                 811\n",
            "(covid, vaccine)             745\n",
            "(covid, trump)               687\n",
            "(covid, case)                686\n",
            "(test, covid)                685\n",
            "(new, covid)                 683\n",
            "(test, positive)             578\n",
            "(trump, say)                 564\n",
            "(say, coronavirus)           509\n",
            "(covid, president)           499\n",
            "(new, case)                  498\n",
            "(trump, coronavirus)         469\n",
            "(president, say)             447\n",
            "(white, house)               413\n",
            "(trump, test)                403\n",
            "(covid, positive)            386\n",
            "(report, case)               372\n",
            "(case, coronavirus)          371\n",
            "(president, coronavirus)     369\n",
            "(new, coronavirus)           363\n",
            "(say, test)                  353\n",
            "(report, covid)              352\n",
            "(test, coronavirus)          328\n",
            "(report, new)                319\n",
            "(vaccine, trial)             309\n",
            "(trump, house)               301\n",
            "(trump, positive)            297\n",
            "(vaccine, coronavirus)       293\n",
            "(president, test)            284\n",
            "\n",
            "China&USA:\n",
            "                          weight\n",
            "(president, trump)          1134\n",
            "(covid, say)                 936\n",
            "(covid, case)                838\n",
            "(new, covid)                 800\n",
            "(covid, vaccine)             798\n",
            "(covid, test)                764\n",
            "(covid, trump)               750\n",
            "(test, positive)             697\n",
            "(new, case)                  679\n",
            "(trump, say)                 609\n",
            "(president, covid)           587\n",
            "(report, case)               555\n",
            "(say, president)             548\n",
            "(coronavirus, say)           547\n",
            "(coronavirus, trump)         493\n",
            "(white, house)               473\n",
            "(china, say)                 468\n",
            "(trump, test)                458\n",
            "(china, covid)               454\n",
            "(positive, covid)            449\n",
            "(report, covid)              446\n",
            "(new, report)                443\n",
            "(case, coronavirus)          434\n",
            "(new, coronavirus)           421\n",
            "(coronavirus, president)     399\n",
            "(test, say)                  394\n",
            "(test, coronavirus)          369\n",
            "(covid, pandemic)            350\n",
            "(trial, vaccine)             339\n",
            "(trump, positive)            330\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph\n"
      ],
      "metadata": {
        "id": "gfvo8x0Ku78D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph(network):\n",
        "  up_weighted = []\n",
        "  for edge in network:\n",
        "      #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
        "      up_weighted.append((edge[0],edge[1],network[edge]))\n",
        "      \n",
        "  #print(network)\n",
        "  #print(up_weighted[0:10])\n",
        "  G = nx.Graph()\n",
        "  G.add_weighted_edges_from(up_weighted)\n",
        "  return G"
      ],
      "metadata": {
        "id": "2NGKuSuYAo-K"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_China = get_graph(network_China)\n",
        "G_USA = get_graph(network_USA)\n",
        "G_China_USA = get_graph(network_China_USA)"
      ],
      "metadata": {
        "id": "TbpTF19bBKJq"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print('Nodes: ',len(G_China.nodes()))\n",
        "print('Edges: ',len(G_China.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China))\n",
        "print()\n",
        "print('USA:')\n",
        "print('Nodes: ',len(G_USA.nodes()))\n",
        "print('Edges: ',len(G_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_USA))\n",
        "print()\n",
        "print('China&USA:')\n",
        "print('Nodes: ',len(G_China_USA.nodes()))\n",
        "print('Edges: ',len(G_China_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China_USA))"
      ],
      "metadata": {
        "id": "W178ljb9rM6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1427fa27-e900-4e63-a68f-8dfc380be9cf"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "Nodes:  1407\n",
            "Edges:  80335\n",
            "Is connected:  True\n",
            "\n",
            "USA:\n",
            "Nodes:  2222\n",
            "Edges:  148901\n",
            "Is connected:  True\n",
            "\n",
            "China&USA:\n",
            "Nodes:  2828\n",
            "Edges:  232823\n",
            "Is connected:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PageRank"
      ],
      "metadata": {
        "id": "1LYNr4McLtWD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "P6qJVv9vHIr1"
      },
      "outputs": [],
      "source": [
        "# Calculating the pagerank on graph G, teleportation probability here is 0.15 but since the graph is strongly connected we can set it to zero if we want\n",
        "pr_China = nx.algorithms.pagerank(G_China,alpha = 1)\n",
        "pr_China = dict(sorted(pr_China.items(), key=lambda item: item[1],reverse  = True))\n",
        "\n",
        "pr_USA = nx.algorithms.pagerank(G_USA,alpha = 1)\n",
        "pr_USA = dict(sorted(pr_USA.items(), key=lambda item: item[1],reverse  = True))\n",
        "\n",
        "pr_China_USA = nx.algorithms.pagerank(G_China_USA,alpha = 1)\n",
        "pr_China_USA = dict(sorted(pr_China_USA.items(), key=lambda item: item[1],reverse  = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "BcVoAXxqpqLd"
      },
      "outputs": [],
      "source": [
        "def threshold(vector,threshold):\n",
        "\n",
        "  l = [(el,vector[el]) for el in vector if vector[el] >= threshold ]\n",
        "\n",
        "  return pd.DataFrame(l)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def threshold_reverse(vector,threshold):\n",
        "\n",
        "  l = [(el,vector[el]) for el in vector if vector[el] < threshold ]\n",
        "\n",
        "  return pd.DataFrame(l)"
      ],
      "metadata": {
        "id": "7-UFIOEJL6bY"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cUhquN72cI6",
        "outputId": "de2437c8-e894-4290-98c9-3910cac6c01d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  794\n",
            "\n",
            "first:\n",
            "              0         1\n",
            "0         china  0.051136\n",
            "1         covid  0.018314\n",
            "2           say  0.014814\n",
            "3           new  0.010169\n",
            "4          case  0.009960\n",
            "5           day  0.009110\n",
            "6          year  0.008864\n",
            "7       country  0.008656\n",
            "8          city  0.007800\n",
            "9        report  0.007610\n",
            "10  coronavirus  0.007489\n",
            "11    president  0.007483\n",
            "12     national  0.007045\n",
            "13        world  0.006720\n",
            "14       people  0.006641\n",
            "15     province  0.006114\n",
            "16         test  0.005847\n",
            "17       health  0.005752\n",
            "18     pandemic  0.005589\n",
            "19      million  0.005586\n",
            "20        first  0.005532\n",
            "21       global  0.004785\n",
            "22      holiday  0.004423\n",
            "23       accord  0.004075\n",
            "24         high  0.004054\n",
            "25      vaccine  0.004019\n",
            "26         take  0.003964\n",
            "27  development  0.003921\n",
            "28      poverty  0.003904\n",
            "29          see  0.003827\n",
            "\n",
            "last:\n",
            "               0         1\n",
            "764       garden  0.000312\n",
            "765         even  0.000312\n",
            "766         wall  0.000312\n",
            "767       person  0.000312\n",
            "768        cover  0.000312\n",
            "769      virtual  0.000310\n",
            "770        japan  0.000310\n",
            "771          kit  0.000310\n",
            "772        batch  0.000310\n",
            "773      outside  0.000310\n",
            "774      publish  0.000310\n",
            "775         nine  0.000309\n",
            "776        grand  0.000308\n",
            "777        honor  0.000307\n",
            "778        value  0.000307\n",
            "779        point  0.000307\n",
            "780        lunar  0.000307\n",
            "781    beautiful  0.000305\n",
            "782         hand  0.000305\n",
            "783  electricity  0.000305\n",
            "784      regular  0.000304\n",
            "785         rush  0.000304\n",
            "786          per  0.000304\n",
            "787          mar  0.000304\n",
            "788        owner  0.000302\n",
            "789    eliminate  0.000302\n",
            "790     producer  0.000302\n",
            "791         moon  0.000302\n",
            "792     internal  0.000302\n",
            "793      theater  0.000301\n",
            "\n",
            "USA:  690\n",
            "\n",
            "first:\n",
            "              0         1\n",
            "0         covid  0.038408\n",
            "1   coronavirus  0.023511\n",
            "2           say  0.019989\n",
            "3         trump  0.018025\n",
            "4     president  0.014184\n",
            "5       vaccine  0.012037\n",
            "6           new  0.011733\n",
            "7          test  0.011480\n",
            "8          case  0.009227\n",
            "9         china  0.008758\n",
            "10     pandemic  0.007546\n",
            "11     positive  0.006666\n",
            "12        house  0.006620\n",
            "13       people  0.005691\n",
            "14       report  0.005661\n",
            "15        white  0.005439\n",
            "16        first  0.005140\n",
            "17          day  0.004805\n",
            "18        state  0.004738\n",
            "19       health  0.004712\n",
            "20        month  0.004297\n",
            "21         week  0.004282\n",
            "22        world  0.004248\n",
            "23          one  0.004230\n",
            "24        virus  0.004071\n",
            "25      million  0.003818\n",
            "26      country  0.003805\n",
            "27        trial  0.003669\n",
            "28    infection  0.003649\n",
            "29        death  0.003639\n",
            "\n",
            "last:\n",
            "               0         1\n",
            "660  development  0.000313\n",
            "661  unexplained  0.000313\n",
            "662         fire  0.000312\n",
            "663    difficult  0.000312\n",
            "664        visit  0.000311\n",
            "665      meeting  0.000310\n",
            "666        labor  0.000309\n",
            "667     spending  0.000308\n",
            "668      pacific  0.000308\n",
            "669        alert  0.000307\n",
            "670      foreign  0.000307\n",
            "671          aim  0.000307\n",
            "672         love  0.000307\n",
            "673      weekend  0.000307\n",
            "674     evidence  0.000306\n",
            "675          dos  0.000305\n",
            "676          ill  0.000305\n",
            "677       figure  0.000305\n",
            "678     assembly  0.000304\n",
            "679        agree  0.000304\n",
            "680   infectious  0.000304\n",
            "681       spring  0.000303\n",
            "682       border  0.000303\n",
            "683         wall  0.000302\n",
            "684       emerge  0.000301\n",
            "685      network  0.000301\n",
            "686      bicycle  0.000301\n",
            "687      improve  0.000301\n",
            "688   commission  0.000301\n",
            "689         wide  0.000300\n",
            "\n",
            "China&USA:  724\n",
            "\n",
            "first:\n",
            "              0         1\n",
            "0         covid  0.031716\n",
            "1         china  0.020072\n",
            "2   coronavirus  0.018443\n",
            "3           say  0.017952\n",
            "4         trump  0.013599\n",
            "5     president  0.011890\n",
            "6           new  0.010875\n",
            "7          test  0.009580\n",
            "8       vaccine  0.009412\n",
            "9          case  0.009016\n",
            "10     pandemic  0.006829\n",
            "11       report  0.005946\n",
            "12          day  0.005783\n",
            "13       people  0.005780\n",
            "14     positive  0.005657\n",
            "15        house  0.005174\n",
            "16        first  0.005065\n",
            "17      country  0.004980\n",
            "18        world  0.004818\n",
            "19       health  0.004798\n",
            "20         year  0.004710\n",
            "21        white  0.004276\n",
            "22      million  0.004138\n",
            "23        state  0.004001\n",
            "24          one  0.003972\n",
            "25         city  0.003761\n",
            "26         week  0.003683\n",
            "27        month  0.003449\n",
            "28         take  0.003329\n",
            "29        death  0.003280\n",
            "\n",
            "last:\n",
            "              0         1\n",
            "694      really  0.000321\n",
            "695  initiative  0.000321\n",
            "696      spring  0.000321\n",
            "697       plant  0.000318\n",
            "698       stand  0.000317\n",
            "699      normal  0.000316\n",
            "700      detail  0.000316\n",
            "701       water  0.000313\n",
            "702         car  0.000310\n",
            "703     weekend  0.000310\n",
            "704        hall  0.000309\n",
            "705      decade  0.000309\n",
            "706       enter  0.000308\n",
            "707    approach  0.000308\n",
            "708   agreement  0.000308\n",
            "709      action  0.000308\n",
            "710      bubble  0.000307\n",
            "711       power  0.000307\n",
            "712        road  0.000306\n",
            "713     suspend  0.000305\n",
            "714      inside  0.000305\n",
            "715        main  0.000304\n",
            "716    headline  0.000303\n",
            "717      reject  0.000303\n",
            "718        flag  0.000303\n",
            "719        ever  0.000302\n",
            "720  department  0.000302\n",
            "721    platform  0.000301\n",
            "722      pompeo  0.000301\n",
            "723        roll  0.000300\n"
          ]
        }
      ],
      "source": [
        "print('China: ', len(threshold(pr_China,0.0003)))\n",
        "print()\n",
        "print('first:')\n",
        "print(threshold(pr_China,0.0003).iloc[:30])\n",
        "print()\n",
        "print('last:')\n",
        "print(threshold(pr_China,0.0003).iloc[len(threshold(pr_China,0.0003))-30:])\n",
        "print()\n",
        "print('USA: ', len(threshold(pr_USA,0.0003)))\n",
        "print()\n",
        "print('first:')\n",
        "print(threshold(pr_USA,0.0003).iloc[:30])\n",
        "print()\n",
        "print('last:')\n",
        "print(threshold(pr_USA,0.0003).iloc[len(threshold(pr_USA,0.0003))-30:])\n",
        "print()\n",
        "print('China&USA: ', len(threshold(pr_China_USA,0.0003)))\n",
        "print()\n",
        "print('first:')\n",
        "print(threshold(pr_China_USA,0.0003).iloc[:30])\n",
        "print()\n",
        "print('last:')\n",
        "print(threshold(pr_China_USA,0.0003).iloc[len(threshold(pr_China_USA,0.0003))-30:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqlZ2nZ9qKW"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "gVbC8VFA9pgw"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,1))   # ngram range can be changed to obtain measures regarding n grams instead of single words\n",
        "\n",
        "X_China = tfidf.fit_transform(cleaned_mostfreq_text_China).toarray()    # entry (i,j) if Tfidf measure of word_list[j] in document i\n",
        "word_list_China = tfidf.get_feature_names_out()\n",
        "\n",
        "X_USA = tfidf.fit_transform(cleaned_mostfreq_text_USA).toarray()\n",
        "word_list_USA = tfidf.get_feature_names_out()\n",
        "\n",
        "X_China_USA = tfidf.fit_transform(cleaned_mostfreq_text_China_USA).toarray()\n",
        "word_list_China_USA = tfidf.get_feature_names_out()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "8k8fcvKX-yah"
      },
      "outputs": [],
      "source": [
        "tfidf_df_China = pd.DataFrame(X_China,columns = word_list_China)\n",
        "\n",
        "tfidf_df_USA = pd.DataFrame(X_USA,columns = word_list_USA)\n",
        "\n",
        "tfidf_df_China_USA = pd.DataFrame(X_China_USA,columns = word_list_China_USA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "MUr9_93WCKsj"
      },
      "outputs": [],
      "source": [
        "tfidf_word_measure_China = np.mean(tfidf_df_China,axis = 0)\n",
        "tfidf_word_measure_China = tfidf_word_measure_China.sort_values(ascending = False)\n",
        "tfidf_word_measure_USA = np.mean(tfidf_df_USA,axis = 0)\n",
        "tfidf_word_measure_USA = tfidf_word_measure_USA.sort_values(ascending = False)\n",
        "tfidf_word_measure_China_USA = np.mean(tfidf_df_China_USA,axis = 0)\n",
        "tfidf_word_measure_China_USA = tfidf_word_measure_China_USA.sort_values(ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(tfidf_word_measure_China[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(tfidf_word_measure_USA[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(tfidf_word_measure_China_USA[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dknv21f3XimF",
        "outputId": "880e96e6-bc8f-414a-e3d4-7509caec4494"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "china          0.057393\n",
            "covid          0.032344\n",
            "case           0.029077\n",
            "novel          0.027626\n",
            "coronavirus    0.027115\n",
            "new            0.025339\n",
            "vaccine        0.022859\n",
            "hospital       0.022712\n",
            "say            0.020808\n",
            "outbreak       0.019470\n",
            "report         0.017295\n",
            "fight          0.016957\n",
            "patient        0.016629\n",
            "people         0.015936\n",
            "confirm        0.014500\n",
            "watch          0.014347\n",
            "health         0.014288\n",
            "country        0.014000\n",
            "day            0.013577\n",
            "live           0.013453\n",
            "province       0.012905\n",
            "first          0.012837\n",
            "city           0.012793\n",
            "death          0.012317\n",
            "year           0.012240\n",
            "world          0.011623\n",
            "epidemic       0.011616\n",
            "amid           0.011082\n",
            "medical        0.011039\n",
            "president      0.010959\n",
            "dtype: float64\n",
            "\n",
            "USA:\n",
            "coronavirus    0.049972\n",
            "covid          0.048809\n",
            "china          0.041930\n",
            "vaccine        0.031179\n",
            "case           0.028555\n",
            "say            0.027612\n",
            "new            0.026551\n",
            "report         0.017860\n",
            "death          0.017719\n",
            "outbreak       0.017477\n",
            "virus          0.016571\n",
            "test           0.016000\n",
            "spread         0.014537\n",
            "people         0.013405\n",
            "health         0.013065\n",
            "trump          0.012863\n",
            "first          0.012383\n",
            "late           0.012190\n",
            "president      0.011938\n",
            "day            0.011102\n",
            "million        0.010978\n",
            "state          0.010971\n",
            "rise           0.010859\n",
            "country        0.010299\n",
            "world          0.009807\n",
            "positive       0.009792\n",
            "pandemic       0.009256\n",
            "infection      0.009202\n",
            "hit            0.008676\n",
            "confirm        0.008295\n",
            "dtype: float64\n",
            "\n",
            "China&USA:\n",
            "china          0.045033\n",
            "coronavirus    0.043029\n",
            "covid          0.042957\n",
            "case           0.027942\n",
            "vaccine        0.027795\n",
            "new            0.025423\n",
            "say            0.024511\n",
            "outbreak       0.017291\n",
            "report         0.017268\n",
            "death          0.015774\n",
            "virus          0.013796\n",
            "people         0.013548\n",
            "test           0.013425\n",
            "health         0.013033\n",
            "first          0.012092\n",
            "novel          0.011916\n",
            "hospital       0.011775\n",
            "spread         0.011651\n",
            "day            0.011465\n",
            "president      0.011090\n",
            "country        0.011073\n",
            "late           0.010992\n",
            "million        0.010481\n",
            "trump          0.010241\n",
            "confirm        0.010155\n",
            "world          0.009889\n",
            "state          0.009267\n",
            "infection      0.009075\n",
            "city           0.009014\n",
            "patient        0.008786\n",
            "dtype: float64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reduced graph"
      ],
      "metadata": {
        "id": "4nkLA8k0LB7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# less frequent words:\n",
        "less_important_words_China = [key for key in list(threshold_reverse(pr_China,0.0003)[0])]\n",
        "\n",
        "less_important_words_USA = [key for key in list(threshold_reverse(pr_USA,0.0003)[0])]\n",
        "\n",
        "less_important_words_China_USA = [key for key in list(threshold_reverse(pr_China_USA,0.0003)[0])]"
      ],
      "metadata": {
        "id": "6ndwCL9oLpDA"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# discard less frequent words\n",
        "cleaned_mostimp_text_China = cleaned_mostfreq_text_China.copy()\n",
        "for txt in range(len(cleaned_mostimp_text_China)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostimp_text_China))\n",
        "  for word in less_important_words_China:\n",
        "    if word in cleaned_mostimp_text_China[txt].split():\n",
        "      cleaned_mostimp_text_China[txt] = cleaned_mostimp_text_China[txt].replace(word, '')\n",
        "      cleaned_mostimp_text_China[txt] = \" \".join(cleaned_mostimp_text_China[txt].split())\n",
        "\n",
        "cleaned_mostimp_text_USA = cleaned_mostfreq_text_USA.copy()\n",
        "for txt in range(len(cleaned_mostimp_text_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostimp_text_USA))\n",
        "  for word in less_important_words_USA:\n",
        "    if word in cleaned_mostimp_text_USA[txt].split():\n",
        "      cleaned_mostimp_text_USA[txt] = cleaned_mostimp_text_USA[txt].replace(word, '')\n",
        "      cleaned_mostimp_text_USA[txt] = \" \".join(cleaned_mostimp_text_USA[txt].split())\n",
        "\n",
        "cleaned_mostimp_text_China_USA = cleaned_mostfreq_text_China_USA.copy()\n",
        "for txt in range(len(cleaned_mostimp_text_China_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostimp_text_China_USA))\n",
        "  for word in less_important_words_China_USA:\n",
        "    if word in cleaned_mostimp_text_China_USA[txt].split():\n",
        "      cleaned_mostimp_text_China_USA[txt] = cleaned_mostimp_text_China_USA[txt].replace(word, '')\n",
        "      cleaned_mostimp_text_China_USA[txt] = \" \".join(cleaned_mostimp_text_China_USA[txt].split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e63cd1-f457-485b-eb9b-15555a35e602",
        "id": "D8L4ETxALpDB"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 3089\n",
            "1000 / 3089\n",
            "2000 / 3089\n",
            "3000 / 3089\n",
            "0 / 8384\n",
            "1000 / 8384\n",
            "2000 / 8384\n",
            "3000 / 8384\n",
            "4000 / 8384\n",
            "5000 / 8384\n",
            "6000 / 8384\n",
            "7000 / 8384\n",
            "8000 / 8384\n",
            "0 / 11473\n",
            "1000 / 11473\n",
            "2000 / 11473\n",
            "3000 / 11473\n",
            "4000 / 11473\n",
            "5000 / 11473\n",
            "6000 / 11473\n",
            "7000 / 11473\n",
            "8000 / 11473\n",
            "9000 / 11473\n",
            "10000 / 11473\n",
            "11000 / 11473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_mostimp_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_mostimp_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_mostimp_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc655789-11d4-48cb-ee21-b1fe07b31579",
        "id": "X2ObCNtJLpDB"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  804\n",
            "USA:  720\n",
            "China&USA:  771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_China = pd.DataFrame.from_dict(freq_dict_China, orient='index').reset_index()\n",
        "df_China.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China.reset_index(inplace=True)\n",
        "df_China.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_USA = pd.DataFrame.from_dict(freq_dict_USA, orient='index').reset_index()\n",
        "df_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_USA.reset_index(inplace=True)\n",
        "df_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_China_USA = pd.DataFrame.from_dict(freq_dict_China_USA, orient='index').reset_index()\n",
        "df_China_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China_USA.reset_index(inplace=True)\n",
        "df_China_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "print('China')\n",
        "print(df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA')\n",
        "print(df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA')\n",
        "print(df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc77f99e-c839-46f0-98e1-fbf83296fe75",
        "id": "DfrViAqzNnb9"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "           Word  Count\n",
            "0         china   1751\n",
            "1         covid    641\n",
            "2           say    446\n",
            "3          case    324\n",
            "4           new    321\n",
            "5   coronavirus    272\n",
            "6          year    256\n",
            "7       country    256\n",
            "8           day    256\n",
            "9     president    238\n",
            "10       report    231\n",
            "11         city    227\n",
            "12     national    216\n",
            "13       people    211\n",
            "14        world    204\n",
            "15         test    193\n",
            "16     province    192\n",
            "17     pandemic    189\n",
            "18       health    171\n",
            "19        first    167\n",
            "20      million    167\n",
            "21        watch    154\n",
            "22       global    153\n",
            "23      vaccine    148\n",
            "24         live    134\n",
            "25        trump    129\n",
            "26         take    127\n",
            "27          see    124\n",
            "28     positive    122\n",
            "29         high    121\n",
            "\n",
            "USA\n",
            "           Word  Count\n",
            "0         covid   4188\n",
            "1   coronavirus   2203\n",
            "2           say   1662\n",
            "3         trump   1315\n",
            "4           new   1170\n",
            "5       vaccine   1167\n",
            "6         china   1073\n",
            "7          case   1055\n",
            "8     president   1017\n",
            "9          test   1006\n",
            "10     pandemic    602\n",
            "11       report    597\n",
            "12     positive    565\n",
            "13        house    461\n",
            "14          day    436\n",
            "15       people    434\n",
            "16        state    404\n",
            "17        death    395\n",
            "18       health    391\n",
            "19        first    387\n",
            "20      million    364\n",
            "21        white    361\n",
            "22        world    349\n",
            "23    infection    340\n",
            "24        trial    332\n",
            "25        month    332\n",
            "26         week    327\n",
            "27      country    319\n",
            "28         rise    318\n",
            "29          one    315\n",
            "\n",
            "China&USA\n",
            "           Word  Count\n",
            "0         covid   4829\n",
            "1         china   2824\n",
            "2   coronavirus   2475\n",
            "3           say   2108\n",
            "4           new   1491\n",
            "5         trump   1444\n",
            "6          case   1379\n",
            "7       vaccine   1315\n",
            "8     president   1255\n",
            "9          test   1199\n",
            "10       report    823\n",
            "11     pandemic    791\n",
            "12          day    692\n",
            "13     positive    687\n",
            "14       people    645\n",
            "15      country    575\n",
            "16       health    562\n",
            "17        first    554\n",
            "18        world    553\n",
            "19      million    531\n",
            "20        house    526\n",
            "21         year    525\n",
            "22        state    491\n",
            "23        death    488\n",
            "24          one    429\n",
            "25         city    426\n",
            "26        white    418\n",
            "27    infection    415\n",
            "28       global    402\n",
            "29         week    400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_China = freq_dict_China.keys()  \n",
        "keys_USA = freq_dict_USA.keys()  \n",
        "keys_China_USA = freq_dict_China_USA.keys()  "
      ],
      "metadata": {
        "id": "Q52SCkkKNncF"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_China, network_df_China = create_network(cleaned_mostimp_text_China)\n",
        "network_USA, network_df_USA = create_network(cleaned_mostimp_text_USA)\n",
        "network_China_USA, network_df_China_USA = create_network(cleaned_mostimp_text_China_USA)"
      ],
      "metadata": {
        "id": "bPn3tfv7NncF"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(network_df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(network_df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(network_df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c554e58d-6f16-4503-9cc9-ba6e335a9933",
        "id": "MVggBElVNncF"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "                      weight\n",
            "(china, say)             283\n",
            "(year, china)            218\n",
            "(china, covid)           212\n",
            "(china, day)             211\n",
            "(china, national)        185\n",
            "(report, case)           183\n",
            "(new, case)              181\n",
            "(national, day)          170\n",
            "(china, country)         152\n",
            "(covid, case)            152\n",
            "(china, province)        151\n",
            "(china, city)            149\n",
            "(poverty, china)         127\n",
            "(new, china)             125\n",
            "(covid, say)             125\n",
            "(new, report)            124\n",
            "(people, china)          121\n",
            "(china, world)           120\n",
            "(day, holiday)           120\n",
            "(test, positive)         117\n",
            "(new, covid)             117\n",
            "(president, china)       111\n",
            "(china, xinjiang)        110\n",
            "(china, holiday)         108\n",
            "(covid, pandemic)        108\n",
            "(china, development)     108\n",
            "(first, china)           105\n",
            "(global, china)          104\n",
            "(say, president)         101\n",
            "(president, trump)        97\n",
            "\n",
            "USA:\n",
            "                          weight\n",
            "(president, trump)          1037\n",
            "(say, covid)                 811\n",
            "(covid, vaccine)             743\n",
            "(covid, trump)               687\n",
            "(covid, case)                686\n",
            "(test, covid)                685\n",
            "(new, covid)                 683\n",
            "(test, positive)             578\n",
            "(trump, say)                 564\n",
            "(say, coronavirus)           509\n",
            "(covid, president)           499\n",
            "(new, case)                  498\n",
            "(trump, coronavirus)         469\n",
            "(president, say)             447\n",
            "(white, house)               413\n",
            "(trump, test)                403\n",
            "(covid, positive)            386\n",
            "(case, coronavirus)          371\n",
            "(report, case)               370\n",
            "(president, coronavirus)     369\n",
            "(new, coronavirus)           363\n",
            "(say, test)                  353\n",
            "(report, covid)              352\n",
            "(test, coronavirus)          328\n",
            "(report, new)                319\n",
            "(vaccine, trial)             309\n",
            "(trump, house)               301\n",
            "(trump, positive)            297\n",
            "(vaccine, coronavirus)       293\n",
            "(trump, white)               284\n",
            "\n",
            "China&USA:\n",
            "                          weight\n",
            "(president, trump)          1134\n",
            "(covid, say)                 936\n",
            "(covid, case)                838\n",
            "(new, covid)                 800\n",
            "(covid, vaccine)             796\n",
            "(covid, test)                764\n",
            "(covid, trump)               750\n",
            "(test, positive)             695\n",
            "(new, case)                  679\n",
            "(trump, say)                 609\n",
            "(president, covid)           587\n",
            "(report, case)               548\n",
            "(say, president)             548\n",
            "(coronavirus, say)           547\n",
            "(coronavirus, trump)         493\n",
            "(white, house)               473\n",
            "(china, say)                 468\n",
            "(trump, test)                458\n",
            "(china, covid)               454\n",
            "(positive, covid)            447\n",
            "(new, report)                443\n",
            "(report, covid)              442\n",
            "(case, coronavirus)          434\n",
            "(new, coronavirus)           421\n",
            "(coronavirus, president)     399\n",
            "(test, say)                  394\n",
            "(test, coronavirus)          369\n",
            "(covid, pandemic)            350\n",
            "(trial, vaccine)             339\n",
            "(trump, positive)            330\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph(network):\n",
        "  up_weighted = []\n",
        "  for edge in network:\n",
        "      #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
        "      up_weighted.append((edge[0],edge[1],network[edge]))\n",
        "      \n",
        "  #print(network)\n",
        "  #print(up_weighted[0:10])\n",
        "  G = nx.Graph()\n",
        "  G.add_weighted_edges_from(up_weighted)\n",
        "  return G"
      ],
      "metadata": {
        "id": "fmQTFAb3NncF"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_China = get_graph(network_China)\n",
        "G_USA = get_graph(network_USA)\n",
        "G_China_USA = get_graph(network_China_USA)"
      ],
      "metadata": {
        "id": "zNrH5boVNncF"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print('Nodes: ',len(G_China.nodes()))\n",
        "print('Edges: ',len(G_China.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China))\n",
        "print()\n",
        "print('USA:')\n",
        "print('Nodes: ',len(G_USA.nodes()))\n",
        "print('Edges: ',len(G_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_USA))\n",
        "print()\n",
        "print('China&USA:')\n",
        "print('Nodes: ',len(G_China_USA.nodes()))\n",
        "print('Edges: ',len(G_China_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China_USA))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a847f20-2d69-4058-f91a-f0b9b2cc6b5f",
        "id": "kx5041hFNncF"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "Nodes:  803\n",
            "Edges:  53298\n",
            "Is connected:  True\n",
            "\n",
            "USA:\n",
            "Nodes:  720\n",
            "Edges:  68156\n",
            "Is connected:  True\n",
            "\n",
            "China&USA:\n",
            "Nodes:  771\n",
            "Edges:  90251\n",
            "Is connected:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save edge list"
      ],
      "metadata": {
        "id": "p4SvWlzSfzFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = './edgelist_China_2_700.csv'\n",
        "nx.write_weighted_edgelist(G_China, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_China_2_700.csv\n",
        "files.download(\"edgelist_China_2_700.csv\")\n",
        "\n",
        "filename = './edgelist_USA_2_700.csv'\n",
        "nx.write_weighted_edgelist(G_USA, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_USA_2_700.csv\n",
        "files.download(\"edgelist_USA_2_700.csv\")\n",
        "\n",
        "filename = './edgelist_China_USA_2_700.csv'\n",
        "nx.write_weighted_edgelist(G_China_USA, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_China_USA_2_700.csv\n",
        "files.download(\"edgelist_China_USA_2_700.csv\")"
      ],
      "metadata": {
        "id": "zgmmRGRCrM9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "13d3e608-474b-4845-8e8d-a6de89e02b34"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5055abda-2f1a-4abf-92d4-95b50de806f2\", \"edgelist_China_2_700.csv\", 844393)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_dd154dfd-467a-4263-9f69-9524e9cb3b83\", \"edgelist_USA_2_700.csv\", 1048392)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f56e074a-3a7a-4f27-bec4-f1a196190f9d\", \"edgelist_China_USA_2_700.csv\", 1398903)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create Node List\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nWHLO7AhdwzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nodes(freq_dict, name):\n",
        "  word_nodes = pd.DataFrame.from_dict(freq_dict,orient=\"index\")\n",
        "  word_nodes.reset_index(inplace=True)\n",
        "  word_nodes[\"Label\"] = word_nodes[\"index\"]\n",
        "  word_nodes.rename(columns={\"index\":\"Id\",0:\"delete\"},inplace=True)\n",
        "  word_nodes = word_nodes.drop(columns=['delete'])\n",
        "  nodelist = pd.DataFrame()\n",
        "  nodelist = nodelist.append(word_nodes, ignore_index=True)\n",
        "\n",
        "  nodelist = nodelist.to_csv(\"nodelist_\"+name+\".csv\",index=False)\n",
        "  files.download(\"nodelist_\"+name+\".csv\")\n",
        "  return nodelist, word_nodes"
      ],
      "metadata": {
        "id": "v2GYb2BQFzET"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodelist_China, word_nodes_China = nodes(freq_dict_China,\"China_2_700\")\n",
        "nodelist_USA, word_nodes_USA = nodes(freq_dict_USA,\"USA_2_700\")\n",
        "nodelist_China_USA, word_nodes_China_USA = nodes(freq_dict_China_USA,\"China_2_USA_700\")\n",
        "\n",
        "print('China:')\n",
        "print(word_nodes_China.head())\n",
        "print()\n",
        "print('USA:')\n",
        "print(word_nodes_USA.head())\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(word_nodes_China_USA.head())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "EmQrEHV0F1QY",
        "outputId": "9ad589e0-2e84-4b7e-babf-51de46af005a"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0fe02c84-fd7d-413c-bc00-df6c281c3560\", \"nodelist_China_2_700.csv\", 11327)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_dcba79f6-a189-41ef-a09f-5a8f7f20d234\", \"nodelist_USA_2_700.csv\", 9723)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_96d30b6d-41b5-489b-822f-b6c66b5d17a2\", \"nodelist_China_2_USA_700.csv\", 10413)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "      Id  Label\n",
            "0  china  china\n",
            "1  covid  covid\n",
            "2    say    say\n",
            "3   case   case\n",
            "4    new    new\n",
            "\n",
            "USA:\n",
            "            Id        Label\n",
            "0        covid        covid\n",
            "1  coronavirus  coronavirus\n",
            "2          say          say\n",
            "3        trump        trump\n",
            "4          new          new\n",
            "\n",
            "China&USA:\n",
            "            Id        Label\n",
            "0        covid        covid\n",
            "1        china        china\n",
            "2  coronavirus  coronavirus\n",
            "3          say          say\n",
            "4          new          new\n",
            "\n"
          ]
        }
      ]
    }
  ]
}