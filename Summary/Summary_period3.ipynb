{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summary_period3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#notebook to download the csv of edges and nodes of a given network\n",
        "import os\n",
        "import requests \n",
        "import time\n",
        "import string\n",
        "import networkx as nx\n",
        "import itertools\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.corpus import wordnet as wn #importing it\n",
        "from nltk.stem.wordnet import WordNetLemmatizer #importing wordnet lemmatizer\n",
        "from nltk import pos_tag #part-of-speech-tagger\n",
        "from collections import defaultdict #defaultdict returns default value for non-existant keys you try to  access based on the function you passed in the constructor\n",
        "from google.colab import files\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRcCylkZqDPi",
        "outputId": "1f276ac2-d067-4422-98b2-a24dac9c11b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(df):       #extract the text from the tweets and RT\n",
        "                            #works ONLY on .csv file\n",
        "  list_strings = []\n",
        "  for index in range(len(df)):\n",
        "    if index % 1000 == 0:\n",
        "      print(str(index)+' / '+str(len(df)))\n",
        "    text = df.loc[index]['text']                          #if it is nor trucated nor a RT  i take \"text\"\n",
        "    string = -1\n",
        "    if (df.loc[index,\"truncated\"] == True):                 #if it is trucated I take \"extended_tweet\"\n",
        "        string = df.loc[index,\"extended_tweet\"]\n",
        "    if type(df.loc[index,\"retweeted_status\"]) != float:     #if it is a RT I take retweeted_status\n",
        "        string = df.loc[index,\"retweeted_status\"]\n",
        "    if type(string) == str :\n",
        "        if(re.search('full_text\\':(.+?)https',string) != None):     #if I find \"full_text\"\n",
        "          s = re.search('full_text\\':(.+?)https',string).group(1)\n",
        "        if(re.search('text\\':(.+?)https',string)!= None):\n",
        "          s = re.search('text\\':(.+?)https',string).group(1)\n",
        "        else: \n",
        "          continue\n",
        "        list_strings.append(s)\n",
        "        #print(s)         \n",
        "    else:\n",
        "      list_strings.append(text)\n",
        "      #print(text)\n",
        "      \n",
        "\n",
        "  return list_strings"
      ],
      "metadata": {
        "id": "CGa_FVVGqDSJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning, lemmatising and pos tagging tweets\n",
        "\n",
        "nltk.download('words')\n",
        "WORDS = set(nltk.corpus.words.words()) #the last two lines serve to download the corpus of standard English language words\n",
        "nltk.download('stopwords') #downloading stopwords\n",
        "STOP_WORDS = set(nltk.corpus.stopwords.words(\"english\")) #taking the stop words from English language\n",
        "nltk.download('wordnet') #downloading wordnet\n",
        "nltk.download('averaged_perceptron_tagger') #downloading tagger\n",
        "tag_map = defaultdict(lambda : wn.NOUN) #here we define that wn.NOUN is the default value for the dict\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "def lemma_pos_cleaner(tweet):\n",
        "\n",
        "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) # remove mentions\n",
        "    tweet = re.sub(\"#[A-Za-z0-9]+\", \"\",tweet) # remove hashtags\n",
        "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) # remove http links\n",
        "    tweet = \" \".join(tweet.split())\n",
        "    tweet = str.lower(tweet) #to lowercase \n",
        "    tweet = re.sub(\"'\",\" \",tweet) # remove aphostrophe\n",
        "\n",
        "    #basically we use pos_tag function on tokens that we get by applying wordpunct tokenization\n",
        "    #to tweet (it separates all the words and symbols)\n",
        "    #then we pass the token along with it's wordnet pos value that we get from the tag_map dictionary (noun, adjective, verb or adverb) to the lemma function (the WordNetLemmatizer())\n",
        "    lemma_function = WordNetLemmatizer()\n",
        "    tweet = \" \".join(lemma_function.lemmatize(token, tag_map[tag[0]]) for token, tag in nltk.pos_tag(nltk.wordpunct_tokenize(tweet))) #lemmatize\n",
        "  \n",
        "\n",
        "    # francesco: I removed also all 2 letters words and added specific words, words that appears frequently but are discarded because they are not in the english language\n",
        "    SPECIFIC_WORDS = ['virus', 'coronavirus', 'covid19', 'covid', 'trump', 'hubei', 'beijing', 'xinjiang', 'jinping', 'korea', 'xinhua', 'india', 'taiwan','johnson','singapore', 'africa', 'japanese', 'france', 'asian', 'australia', 'french', 'asia', 'leishenshan', 'british', 'qingdao', 'fauci', 'america',  'california', 'sichuan', 'malaysia', 'huawei','thailand', 'shandong', 'italy', 'philippines', 'germany', 'facebook', 'african', 'shenzhen', 'tokyo', 'russian','uygur', '5g', 'pompeo', 'vietnam', 'australian', 'cambodia', 'zhejiang', 'yunnan', 'guangdong', 'korean', 'iran', 'washington']\n",
        "    tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) if (w in WORDS or w in SPECIFIC_WORDS) and len(w)>2 and w not in STOP_WORDS ) #remove stop words\n",
        "   \n",
        "    return tweet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD0LDoYVqDUh",
        "outputId": "8b6ff527-7e43-426f-c9f1-d78ad7b5bb26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def frequency_dictionary(df):\n",
        "  unique_words = {}\n",
        "\n",
        "  for row in df:\n",
        "    for word in row.split():\n",
        "      #if the word is encountered for the first time add to dict as key and set its value to 0\n",
        "      unique_words.setdefault(word,0)\n",
        "      #increase the value (i.e the count) of the word by 1 every time it is encountered\n",
        "      unique_words[word] += 1\n",
        "\n",
        "  return unique_words"
      ],
      "metadata": {
        "id": "MAfOr8vR4P88"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "period = '_MarchApril2021'  # _JanFeb2020, _MarchApril2021, _SeptOct2020"
      ],
      "metadata": {
        "id": "o-GUlzAVOwk_"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "China = pd.read_csv('/content/China'+period+'.csv')\n",
        "USA = pd.read_csv('/content/USA'+period+'.csv')\n",
        "China_USA = pd.read_csv('/content/China&USA'+period+'.csv')"
      ],
      "metadata": {
        "id": "suPdLaUx1Ct8"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of tweets:\n",
        "print('China: ', len(China))\n",
        "print('USA: ', len(USA))\n",
        "print('China&USA: ', len(China_USA))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeO9nxO6Br3x",
        "outputId": "c5b1c989-58d9-4b25-f0af-ba9b9b828d64"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  1376\n",
            "USA:  3957\n",
            "China&USA:  5333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_China = extract_text(China)\n",
        "text_USA = extract_text(USA)\n",
        "text_China_USA = extract_text(China_USA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WI3Rz9T1Cxo",
        "outputId": "b6a2f202-99c8-4360-cb7b-e2c90da1781f"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 1376\n",
            "1000 / 1376\n",
            "0 / 3957\n",
            "1000 / 3957\n",
            "2000 / 3957\n",
            "3000 / 3957\n",
            "0 / 5333\n",
            "1000 / 5333\n",
            "2000 / 5333\n",
            "3000 / 5333\n",
            "4000 / 5333\n",
            "5000 / 5333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text_China = [lemma_pos_cleaner(txt) for txt in text_China]\n",
        "cleaned_text_USA = [lemma_pos_cleaner(txt) for txt in text_USA]\n",
        "cleaned_text_China_USA = [lemma_pos_cleaner(txt) for txt in text_China_USA]\n",
        "\n",
        "print('China:')\n",
        "print(cleaned_text_China[0:10])\n",
        "print()\n",
        "print('USA:')\n",
        "print(cleaned_text_USA[0:10])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(cleaned_text_China_USA[0:10])"
      ],
      "metadata": {
        "id": "6q4JI_jV1C2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66610090-2059-4797-91af-3c8923361474"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "['world health organization express concern rise rate new covid infection death third wave virus sweep across world', 'director general world health organization warn rise global covid case death press conference emphasize public follow covid health guideline contain virus', 'vast majority covid vaccine administer far wealthy nation world health organization report', 'africa administer percent million covid vaccine dos give globally world health organization say', 'causal link covid vaccine rare occurrence blood clot low platelet consider plausible confirm world health organization say', 'mark annual world health day year call fairer healthy world everyone eliminate health inequity pandemic push people poverty food insecurity social problem', 'covid19 vaccine produce expect review emergency use end say accord people daily', 'region currently fight resurgence covid transmission world health organization regional office call upon government region scale vaccine production vaccination', 'exclusive professor yang head joint team molecular trace group explain origin trace study', 'data provide maker show covid vaccine level efficacy compatible requirement world health organization chair advisory panel say']\n",
            "\n",
            "USA:\n",
            "['covid pandemic long way due confusion complacency address disease say world health organization', 'say covid pandemic long way due confusion complacency address disease bring control proven public health measure equitable vaccination', 'virus defeat divided world bank urge unity global vaccine effort', 'travesty country health worker risk group remain completely unvaccinated chief', 'report lack crucial data information access white house long report origin covid say present incomplete picture', 'country include japan raise concern report covid origin', 'joint china study origin covid say transmission virus bat human another animal likely scenario', 'south africa india turn ask vaccine maker give intellectual property right covid vaccine produce locally vaccine maker give away patent speed', 'urge vaccine maker overcome vaccine inequity end covid pandemic follow lead license technology', 'almost dozen country france start use vaccine body quell fear blood clot report']\n",
            "\n",
            "China&USA:\n",
            "['world health organization express concern rise rate new covid infection death third wave virus sweep across world', 'director general world health organization warn rise global covid case death press conference emphasize public follow covid health guideline contain virus', 'vast majority covid vaccine administer far wealthy nation world health organization report', 'africa administer percent million covid vaccine dos give globally world health organization say', 'causal link covid vaccine rare occurrence blood clot low platelet consider plausible confirm world health organization say', 'mark annual world health day year call fairer healthy world everyone eliminate health inequity pandemic push people poverty food insecurity social problem', 'covid19 vaccine produce expect review emergency use end say accord people daily', 'region currently fight resurgence covid transmission world health organization regional office call upon government region scale vaccine production vaccination', 'exclusive professor yang head joint team molecular trace group explain origin trace study', 'data provide maker show covid vaccine level efficacy compatible requirement world health organization chair advisory panel say']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCiw0q7t4UER",
        "outputId": "0fd07088-547b-4833-8379-5f7f1568b45b"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  2005\n",
            "USA:  3334\n",
            "China&USA:  3860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most frequent words\n",
        "print('China')\n",
        "print([key for key in freq_dict_China.keys() if freq_dict_China[key]>200])\n",
        "print()\n",
        "print('USA')\n",
        "print([key for key in freq_dict_USA.keys() if freq_dict_USA[key]>400])\n",
        "print()\n",
        "print('China&USA')\n",
        "print([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]>600])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y29OU9Cd5Auv",
        "outputId": "28fb6d7f-1de5-4510-9422-2fdec7aad690"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "['covid', 'vaccine', 'china']\n",
            "\n",
            "USA\n",
            "['covid', 'vaccine', 'say', 'new']\n",
            "\n",
            "China&USA\n",
            "['covid', 'vaccine', 'say']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# less frequent words"
      ],
      "metadata": {
        "id": "9oHPXYbpINWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# less frequent words:\n",
        "print('Less frequent China: ', len([key for key in freq_dict_China.keys() if freq_dict_China[key]<5]))\n",
        "print('More frequent China: ', len([key for key in freq_dict_China.keys() if freq_dict_China[key]>=5]))\n",
        "less_frequent_words_China = [key for key in freq_dict_China.keys() if freq_dict_China[key]<5]\n",
        "\n",
        "print('Less frequent USA: ', len([key for key in freq_dict_USA.keys() if freq_dict_USA[key]<5]))\n",
        "print('More frequent USA:', len([key for key in freq_dict_USA.keys() if freq_dict_USA[key]>=5]))\n",
        "less_frequent_words_USA = [key for key in freq_dict_USA.keys() if freq_dict_USA[key]<5]\n",
        "\n",
        "print('Less frequent China&USA: ', len([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]<5]))\n",
        "print('More frequent China&USA: ', len([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]>=5]))\n",
        "less_frequent_words_China_USA = [key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]<5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgiQzaHD5zAb",
        "outputId": "278e51d8-39d3-4b57-c645-45f51588e7de"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Less frequent China:  1442\n",
            "More frequent China:  563\n",
            "Less frequent USA:  2094\n",
            "More frequent USA: 1240\n",
            "Less frequent China&USA:  2387\n",
            "More frequent China&USA:  1473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# discard less frequent words\n",
        "cleaned_mostfreq_text_China = cleaned_text_China.copy()\n",
        "for txt in range(len(cleaned_mostfreq_text_China)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostfreq_text_China))\n",
        "  for word in less_frequent_words_China:\n",
        "    if word in cleaned_mostfreq_text_China[txt].split():\n",
        "      cleaned_mostfreq_text_China[txt] = cleaned_mostfreq_text_China[txt].replace(word, '')\n",
        "      cleaned_mostfreq_text_China[txt] = \" \".join(cleaned_mostfreq_text_China[txt].split())\n",
        "\n",
        "cleaned_mostfreq_text_USA = cleaned_text_USA.copy()\n",
        "for txt in range(len(cleaned_mostfreq_text_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostfreq_text_USA))\n",
        "  for word in less_frequent_words_USA:\n",
        "    if word in cleaned_mostfreq_text_USA[txt].split():\n",
        "      cleaned_mostfreq_text_USA[txt] = cleaned_mostfreq_text_USA[txt].replace(word, '')\n",
        "      cleaned_mostfreq_text_USA[txt] = \" \".join(cleaned_mostfreq_text_USA[txt].split())\n",
        "\n",
        "cleaned_mostfreq_text_China_USA = cleaned_text_China_USA.copy()\n",
        "for txt in range(len(cleaned_mostfreq_text_China_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostfreq_text_China_USA))\n",
        "  for word in less_frequent_words_China_USA:\n",
        "    if word in cleaned_mostfreq_text_China_USA[txt].split():\n",
        "      cleaned_mostfreq_text_China_USA[txt] = cleaned_mostfreq_text_China_USA[txt].replace(word, '')\n",
        "      cleaned_mostfreq_text_China_USA[txt] = \" \".join(cleaned_mostfreq_text_China_USA[txt].split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmlqtzHp6lh8",
        "outputId": "529d6026-0361-437a-c679-52c19e450464"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 1336\n",
            "1000 / 1336\n",
            "0 / 3957\n",
            "1000 / 3957\n",
            "2000 / 3957\n",
            "3000 / 3957\n",
            "0 / 5293\n",
            "1000 / 5293\n",
            "2000 / 5293\n",
            "3000 / 5293\n",
            "4000 / 5293\n",
            "5000 / 5293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_mostfreq_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_mostfreq_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_mostfreq_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KHj5-o57f8y",
        "outputId": "0e60b03a-5c63-4551-cb00-e1657f2ef816"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  568\n",
            "USA:  1246\n",
            "China&USA:  1484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Network"
      ],
      "metadata": {
        "id": "1davLfA3HNV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_China = pd.DataFrame.from_dict(freq_dict_China, orient='index').reset_index()\n",
        "df_China.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China.reset_index(inplace=True)\n",
        "df_China.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_USA = pd.DataFrame.from_dict(freq_dict_USA, orient='index').reset_index()\n",
        "df_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_USA.reset_index(inplace=True)\n",
        "df_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_China_USA = pd.DataFrame.from_dict(freq_dict_China_USA, orient='index').reset_index()\n",
        "df_China_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China_USA.reset_index(inplace=True)\n",
        "df_China_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "print('China')\n",
        "print(df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA')\n",
        "print(df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA')\n",
        "print(df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxPPnevj890b",
        "outputId": "fd44fc89-d62a-4cc5-d9a7-4a9d848e5869"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "           Word  Count\n",
            "0         covid    959\n",
            "1       vaccine    767\n",
            "2         china    328\n",
            "3           say    191\n",
            "4        report    175\n",
            "5        health    173\n",
            "6          case    172\n",
            "7       country    172\n",
            "8       receive    159\n",
            "9         first    156\n",
            "10      million    151\n",
            "11          new    150\n",
            "12          dos    123\n",
            "13       people    120\n",
            "14     pandemic    113\n",
            "15        batch    105\n",
            "16        world     96\n",
            "17    president     77\n",
            "18       accord     72\n",
            "19         dose     71\n",
            "20          use     70\n",
            "21     national     67\n",
            "22        death     65\n",
            "23          day     65\n",
            "24  vaccination     65\n",
            "25          one     64\n",
            "26         year     63\n",
            "27       brazil     63\n",
            "28       second     59\n",
            "29   administer     59\n",
            "\n",
            "USA\n",
            "           Word  Count\n",
            "0         covid   3021\n",
            "1       vaccine   2128\n",
            "2           say    879\n",
            "3           new    413\n",
            "4          case    395\n",
            "5       johnson    298\n",
            "6          shot    259\n",
            "7       million    258\n",
            "8        report    253\n",
            "9           use    253\n",
            "10       people    247\n",
            "11          get    247\n",
            "12      country    247\n",
            "13        india    238\n",
            "14       health    235\n",
            "15        first    235\n",
            "16          dos    220\n",
            "17        blood    217\n",
            "18  vaccination    216\n",
            "19         clot    215\n",
            "20     pandemic    213\n",
            "21        death    211\n",
            "22  coronavirus    210\n",
            "23        state    207\n",
            "24         year    177\n",
            "25       brazil    167\n",
            "26        surge    157\n",
            "27    president    148\n",
            "28        world    143\n",
            "29    infection    142\n",
            "\n",
            "China&USA\n",
            "           Word  Count\n",
            "0         covid   3980\n",
            "1       vaccine   2895\n",
            "2           say   1070\n",
            "3          case    567\n",
            "4           new    563\n",
            "5         china    460\n",
            "6        report    428\n",
            "7       country    419\n",
            "8       million    409\n",
            "9        health    408\n",
            "10        first    391\n",
            "11       people    367\n",
            "12      johnson    346\n",
            "13          dos    343\n",
            "14     pandemic    326\n",
            "15          use    323\n",
            "16         shot    290\n",
            "17      receive    286\n",
            "18          get    285\n",
            "19  vaccination    281\n",
            "20        death    276\n",
            "21        state    262\n",
            "22        blood    258\n",
            "23  coronavirus    253\n",
            "24        india    249\n",
            "25         clot    247\n",
            "26         year    240\n",
            "27        world    239\n",
            "28       brazil    230\n",
            "29    president    225\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_China = freq_dict_China.keys()  \n",
        "keys_USA = freq_dict_USA.keys()  \n",
        "keys_China_USA = freq_dict_China_USA.keys()  "
      ],
      "metadata": {
        "id": "zHmEpbLtr46D"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_network(cleaned_text):\n",
        "  network = {}\n",
        "  #connect the word that appear in the same tweets\n",
        "  for row in cleaned_text:\n",
        "    combined_list = [word for word in str.split(row)]\n",
        "    #for pair in itertools.product(combined_list, combined_list):\n",
        "    #print(combined_list)\n",
        "    for pair in itertools.product(combined_list, combined_list):\n",
        "          #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
        "          if pair[0]!=pair[1] and not(pair[::-1] in network):\n",
        "              network.setdefault(pair,0)\n",
        "              network[pair] += 1 \n",
        "  network_df = pd.DataFrame.from_dict(network, orient=\"index\")\n",
        "  network_df.columns = [\"weight\"]\n",
        "  network_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
        "  return network, network_df"
      ],
      "metadata": {
        "id": "nUAWB6lT1DAd"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_China, network_df_China = create_network(cleaned_mostfreq_text_China)\n",
        "network_USA, network_df_USA = create_network(cleaned_mostfreq_text_USA)\n",
        "network_China_USA, network_df_China_USA = create_network(cleaned_mostfreq_text_China_USA)"
      ],
      "metadata": {
        "id": "7vR_r37vAGye"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(network_df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(network_df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(network_df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxMWDZua_b-l",
        "outputId": "157bb25c-ca46-478b-bb9c-07996f02d6e2"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "                    weight\n",
            "(covid, vaccine)       529\n",
            "(vaccine, china)       253\n",
            "(china, covid)         249\n",
            "(receive, vaccine)     171\n",
            "(covid, case)          143\n",
            "(million, covid)       141\n",
            "(health, covid)        139\n",
            "(vaccine, say)         137\n",
            "(covid, say)           135\n",
            "(vaccine, dos)         135\n",
            "(country, vaccine)     134\n",
            "(covid, country)       127\n",
            "(new, covid)           125\n",
            "(first, vaccine)       125\n",
            "(covid, report)        121\n",
            "(first, covid)         119\n",
            "(receive, covid)       118\n",
            "(million, vaccine)     113\n",
            "(vaccine, health)      110\n",
            "(batch, vaccine)       109\n",
            "(covid, dos)           105\n",
            "(covid, pandemic)      103\n",
            "(new, case)             98\n",
            "(covid, people)         92\n",
            "(batch, covid)          90\n",
            "(million, dos)          90\n",
            "(vaccine, use)          83\n",
            "(case, report)          81\n",
            "(world, covid)          74\n",
            "(dose, vaccine)         73\n",
            "\n",
            "USA:\n",
            "                      weight\n",
            "(vaccine, covid)        1459\n",
            "(covid, say)             649\n",
            "(vaccine, say)           633\n",
            "(new, covid)             363\n",
            "(covid, case)            354\n",
            "(johnson, vaccine)       287\n",
            "(johnson, covid)         239\n",
            "(use, vaccine)           238\n",
            "(vaccine, clot)          234\n",
            "(vaccine, blood)         233\n",
            "(vaccine, dos)           224\n",
            "(report, covid)          223\n",
            "(million, covid)         198\n",
            "(blood, clot)            197\n",
            "(country, covid)         196\n",
            "(covid, death)           195\n",
            "(covid, shot)            193\n",
            "(covid, first)           192\n",
            "(india, covid)           185\n",
            "(covid, get)             185\n",
            "(million, vaccine)       184\n",
            "(use, covid)             183\n",
            "(covid, health)          182\n",
            "(covid, dos)             174\n",
            "(people, covid)          165\n",
            "(shot, vaccine)          163\n",
            "(state, covid)           162\n",
            "(covid, vaccination)     162\n",
            "(get, vaccine)           160\n",
            "(country, vaccine)       152\n",
            "\n",
            "China&USA:\n",
            "                    weight\n",
            "(covid, vaccine)      1988\n",
            "(covid, say)           784\n",
            "(vaccine, say)         770\n",
            "(covid, case)          497\n",
            "(new, covid)           488\n",
            "(china, covid)         363\n",
            "(vaccine, dos)         359\n",
            "(covid, report)        344\n",
            "(johnson, vaccine)     343\n",
            "(million, covid)       339\n",
            "(vaccine, china)       325\n",
            "(covid, country)       323\n",
            "(vaccine, use)         321\n",
            "(health, covid)        321\n",
            "(first, covid)         311\n",
            "(million, vaccine)     297\n",
            "(country, vaccine)     286\n",
            "(receive, vaccine)     280\n",
            "(covid, dos)           279\n",
            "(vaccine, blood)       276\n",
            "(vaccine, clot)        268\n",
            "(covid, people)        257\n",
            "(johnson, covid)       257\n",
            "(covid, death)         256\n",
            "(first, vaccine)       251\n",
            "(covid, pandemic)      250\n",
            "(vaccine, health)      249\n",
            "(use, covid)           234\n",
            "(receive, covid)       233\n",
            "(blood, clot)          233\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph\n"
      ],
      "metadata": {
        "id": "gfvo8x0Ku78D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph(network):\n",
        "  up_weighted = []\n",
        "  for edge in network:\n",
        "      #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
        "      up_weighted.append((edge[0],edge[1],network[edge]))\n",
        "      \n",
        "  #print(network)\n",
        "  #print(up_weighted[0:10])\n",
        "  G = nx.Graph()\n",
        "  G.add_weighted_edges_from(up_weighted)\n",
        "  return G"
      ],
      "metadata": {
        "id": "2NGKuSuYAo-K"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_China = get_graph(network_China)\n",
        "G_USA = get_graph(network_USA)\n",
        "G_China_USA = get_graph(network_China_USA)"
      ],
      "metadata": {
        "id": "TbpTF19bBKJq"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print('Nodes: ',len(G_China.nodes()))\n",
        "print('Edges: ',len(G_China.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China))\n",
        "print()\n",
        "print('USA:')\n",
        "print('Nodes: ',len(G_USA.nodes()))\n",
        "print('Edges: ',len(G_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_USA))\n",
        "print()\n",
        "print('China&USA:')\n",
        "print('Nodes: ',len(G_China_USA.nodes()))\n",
        "print('Edges: ',len(G_China_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China_USA))"
      ],
      "metadata": {
        "id": "W178ljb9rM6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dff3046-2126-4999-e232-3feeaffc86c1"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "Nodes:  568\n",
            "Edges:  19887\n",
            "Is connected:  True\n",
            "\n",
            "USA:\n",
            "Nodes:  1246\n",
            "Edges:  63970\n",
            "Is connected:  True\n",
            "\n",
            "China&USA:\n",
            "Nodes:  1484\n",
            "Edges:  87741\n",
            "Is connected:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PageRank"
      ],
      "metadata": {
        "id": "1LYNr4McLtWD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "P6qJVv9vHIr1"
      },
      "outputs": [],
      "source": [
        "# Calculating the pagerank on graph G, teleportation probability here is 0.15 but since the graph is strongly connected we can set it to zero if we want\n",
        "pr_China = nx.algorithms.pagerank(G_China,alpha = 1)\n",
        "pr_China = dict(sorted(pr_China.items(), key=lambda item: item[1],reverse  = True))\n",
        "\n",
        "pr_USA = nx.algorithms.pagerank(G_USA,alpha = 1)\n",
        "pr_USA = dict(sorted(pr_USA.items(), key=lambda item: item[1],reverse  = True))\n",
        "\n",
        "pr_China_USA = nx.algorithms.pagerank(G_China_USA,alpha = 1)\n",
        "pr_China_USA = dict(sorted(pr_China_USA.items(), key=lambda item: item[1],reverse  = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "BcVoAXxqpqLd"
      },
      "outputs": [],
      "source": [
        "def threshold(vector,threshold):\n",
        "\n",
        "  l = [(el,vector[el]) for el in vector if vector[el] >= threshold ]\n",
        "\n",
        "  return pd.DataFrame(l)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def threshold_reverse(vector,threshold):\n",
        "\n",
        "  l = [(el,vector[el]) for el in vector if vector[el] < threshold ]\n",
        "\n",
        "  return pd.DataFrame(l)"
      ],
      "metadata": {
        "id": "7-UFIOEJL6bY"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cUhquN72cI6",
        "outputId": "b77f0d28-1bb7-4623-d7be-c290f9296cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  552\n",
            "\n",
            "first:\n",
            "               0         1\n",
            "0          covid  0.070439\n",
            "1        vaccine  0.055125\n",
            "2          china  0.026009\n",
            "3            say  0.017835\n",
            "4         health  0.016636\n",
            "5        country  0.015820\n",
            "6         report  0.015076\n",
            "7           case  0.013800\n",
            "8        million  0.012868\n",
            "9          first  0.012644\n",
            "10       receive  0.011891\n",
            "11           new  0.011646\n",
            "12           dos  0.010449\n",
            "13        people  0.010419\n",
            "14         world  0.009165\n",
            "15      pandemic  0.008211\n",
            "16        accord  0.007000\n",
            "17         batch  0.006803\n",
            "18      national  0.006340\n",
            "19           day  0.006168\n",
            "20         death  0.005893\n",
            "21           use  0.005830\n",
            "22     president  0.005761\n",
            "23   vaccination  0.005744\n",
            "24        brazil  0.005573\n",
            "25          year  0.005367\n",
            "26          dose  0.005337\n",
            "27    administer  0.005242\n",
            "28  organization  0.005153\n",
            "29        origin  0.005108\n",
            "\n",
            "last:\n",
            "               0         1\n",
            "522     business  0.000380\n",
            "523         slow  0.000380\n",
            "524  development  0.000380\n",
            "525         bank  0.000380\n",
            "526       affect  0.000372\n",
            "527         pick  0.000372\n",
            "528      tourist  0.000372\n",
            "529     immunize  0.000372\n",
            "530       impose  0.000372\n",
            "531      federal  0.000371\n",
            "532          lam  0.000371\n",
            "533     zimbabwe  0.000371\n",
            "534       result  0.000363\n",
            "535        count  0.000363\n",
            "536       severe  0.000363\n",
            "537      germany  0.000363\n",
            "538          way  0.000355\n",
            "539        model  0.000354\n",
            "540      related  0.000354\n",
            "541         hong  0.000346\n",
            "542      society  0.000346\n",
            "543    candidate  0.000346\n",
            "544       growth  0.000337\n",
            "545       curfew  0.000337\n",
            "546      british  0.000328\n",
            "547       manila  0.000328\n",
            "548    scientist  0.000320\n",
            "549    highlight  0.000311\n",
            "550         full  0.000311\n",
            "551        probe  0.000303\n",
            "\n",
            "USA:  676\n",
            "\n",
            "first:\n",
            "              0         1\n",
            "0         covid  0.063874\n",
            "1       vaccine  0.047072\n",
            "2           say  0.023262\n",
            "3           new  0.009925\n",
            "4          case  0.009437\n",
            "5       johnson  0.008682\n",
            "6       country  0.007628\n",
            "7        health  0.007251\n",
            "8          shot  0.007151\n",
            "9        people  0.007127\n",
            "10          get  0.006827\n",
            "11  coronavirus  0.006558\n",
            "12       report  0.006503\n",
            "13        blood  0.006497\n",
            "14     pandemic  0.006475\n",
            "15          use  0.006378\n",
            "16         clot  0.006196\n",
            "17        state  0.006024\n",
            "18        first  0.005914\n",
            "19      million  0.005731\n",
            "20  vaccination  0.005676\n",
            "21         year  0.005515\n",
            "22        death  0.005351\n",
            "23        india  0.004982\n",
            "24          dos  0.004762\n",
            "25          day  0.004197\n",
            "26        world  0.004078\n",
            "27        surge  0.004060\n",
            "28    president  0.003914\n",
            "29         risk  0.003880\n",
            "\n",
            "last:\n",
            "                 0         1\n",
            "646           real  0.000323\n",
            "647      authorize  0.000323\n",
            "648        compare  0.000323\n",
            "649         combat  0.000321\n",
            "650         thanks  0.000321\n",
            "651       facility  0.000321\n",
            "652       worrying  0.000321\n",
            "653        account  0.000318\n",
            "654        silence  0.000316\n",
            "655       capacity  0.000316\n",
            "656          rapid  0.000313\n",
            "657            per  0.000313\n",
            "658       passover  0.000313\n",
            "659         exceed  0.000313\n",
            "660           rich  0.000313\n",
            "661      avoidable  0.000313\n",
            "662       shipment  0.000313\n",
            "663      available  0.000313\n",
            "664       military  0.000311\n",
            "665        similar  0.000311\n",
            "666           pope  0.000309\n",
            "667            bar  0.000309\n",
            "668            aid  0.000306\n",
            "669           body  0.000306\n",
            "670           lung  0.000306\n",
            "671  psychological  0.000306\n",
            "672     opposition  0.000304\n",
            "673        discuss  0.000304\n",
            "674        hundred  0.000301\n",
            "675        rapidly  0.000301\n",
            "\n",
            "China&USA:  663\n",
            "\n",
            "first:\n",
            "              0         1\n",
            "0         covid  0.062611\n",
            "1       vaccine  0.046663\n",
            "2           say  0.021079\n",
            "3          case  0.009930\n",
            "4           new  0.009779\n",
            "5       country  0.009155\n",
            "6        health  0.009030\n",
            "7        report  0.008106\n",
            "8        people  0.007628\n",
            "9         china  0.007626\n",
            "10      johnson  0.007189\n",
            "11        first  0.007073\n",
            "12      million  0.006912\n",
            "13     pandemic  0.006804\n",
            "14          use  0.005964\n",
            "15         shot  0.005775\n",
            "16          dos  0.005685\n",
            "17          get  0.005602\n",
            "18  coronavirus  0.005560\n",
            "19        blood  0.005516\n",
            "20        state  0.005506\n",
            "21  vaccination  0.005440\n",
            "22         year  0.005281\n",
            "23        death  0.005209\n",
            "24      receive  0.005127\n",
            "25         clot  0.005123\n",
            "26        world  0.005057\n",
            "27          day  0.004435\n",
            "28    president  0.004203\n",
            "29       accord  0.004075\n",
            "\n",
            "last:\n",
            "                  0         1\n",
            "633           heart  0.000316\n",
            "634           carry  0.000316\n",
            "635        guidance  0.000316\n",
            "636            play  0.000316\n",
            "637      technology  0.000316\n",
            "638             mix  0.000316\n",
            "639       statement  0.000314\n",
            "640       available  0.000314\n",
            "641          normal  0.000313\n",
            "642        progress  0.000313\n",
            "643           tough  0.000313\n",
            "644         science  0.000313\n",
            "645            host  0.000312\n",
            "646           alarm  0.000311\n",
            "647         western  0.000309\n",
            "648      confidence  0.000309\n",
            "649  pharmaceutical  0.000309\n",
            "650       political  0.000308\n",
            "651        discover  0.000307\n",
            "652            fail  0.000307\n",
            "653          immune  0.000307\n",
            "654       extremely  0.000306\n",
            "655            stay  0.000306\n",
            "656         compare  0.000305\n",
            "657         product  0.000304\n",
            "658           weigh  0.000303\n",
            "659            shop  0.000303\n",
            "660        recovery  0.000302\n",
            "661           never  0.000302\n",
            "662            wall  0.000302\n"
          ]
        }
      ],
      "source": [
        "print('China: ', len(threshold(pr_China,0.0003)))\n",
        "print()\n",
        "print('first:')\n",
        "print(threshold(pr_China,0.0003).iloc[:30])\n",
        "print()\n",
        "print('last:')\n",
        "print(threshold(pr_China,0.0003).iloc[len(threshold(pr_China,0.0003))-30:])\n",
        "print()\n",
        "print('USA: ', len(threshold(pr_USA,0.0003)))\n",
        "print()\n",
        "print('first:')\n",
        "print(threshold(pr_USA,0.0003).iloc[:30])\n",
        "print()\n",
        "print('last:')\n",
        "print(threshold(pr_USA,0.0003).iloc[len(threshold(pr_USA,0.0003))-30:])\n",
        "print()\n",
        "print('China&USA: ', len(threshold(pr_China_USA,0.0003)))\n",
        "print()\n",
        "print('first:')\n",
        "print(threshold(pr_China_USA,0.0003).iloc[:30])\n",
        "print()\n",
        "print('last:')\n",
        "print(threshold(pr_China_USA,0.0003).iloc[len(threshold(pr_China_USA,0.0003))-30:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqlZ2nZ9qKW"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "gVbC8VFA9pgw"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,1))   # ngram range can be changed to obtain measures regarding n grams instead of single words\n",
        "\n",
        "X_China = tfidf.fit_transform(cleaned_mostfreq_text_China).toarray()    # entry (i,j) if Tfidf measure of word_list[j] in document i\n",
        "word_list_China = tfidf.get_feature_names_out()\n",
        "\n",
        "X_USA = tfidf.fit_transform(cleaned_mostfreq_text_USA).toarray()\n",
        "word_list_USA = tfidf.get_feature_names_out()\n",
        "\n",
        "X_China_USA = tfidf.fit_transform(cleaned_mostfreq_text_China_USA).toarray()\n",
        "word_list_China_USA = tfidf.get_feature_names_out()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "8k8fcvKX-yah"
      },
      "outputs": [],
      "source": [
        "tfidf_df_China = pd.DataFrame(X_China,columns = word_list_China)\n",
        "\n",
        "tfidf_df_USA = pd.DataFrame(X_USA,columns = word_list_USA)\n",
        "\n",
        "tfidf_df_China_USA = pd.DataFrame(X_China_USA,columns = word_list_China_USA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "MUr9_93WCKsj"
      },
      "outputs": [],
      "source": [
        "tfidf_word_measure_China = np.mean(tfidf_df_China,axis = 0)\n",
        "tfidf_word_measure_China = tfidf_word_measure_China.sort_values(ascending = False)\n",
        "tfidf_word_measure_USA = np.mean(tfidf_df_USA,axis = 0)\n",
        "tfidf_word_measure_USA = tfidf_word_measure_USA.sort_values(ascending = False)\n",
        "tfidf_word_measure_China_USA = np.mean(tfidf_df_China_USA,axis = 0)\n",
        "tfidf_word_measure_China_USA = tfidf_word_measure_China_USA.sort_values(ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(tfidf_word_measure_China[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(tfidf_word_measure_USA[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(tfidf_word_measure_China_USA[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dknv21f3XimF",
        "outputId": "880e96e6-bc8f-414a-e3d4-7509caec4494"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "china          0.057393\n",
            "covid          0.032344\n",
            "case           0.029077\n",
            "novel          0.027626\n",
            "coronavirus    0.027115\n",
            "new            0.025339\n",
            "vaccine        0.022859\n",
            "hospital       0.022712\n",
            "say            0.020808\n",
            "outbreak       0.019470\n",
            "report         0.017295\n",
            "fight          0.016957\n",
            "patient        0.016629\n",
            "people         0.015936\n",
            "confirm        0.014500\n",
            "watch          0.014347\n",
            "health         0.014288\n",
            "country        0.014000\n",
            "day            0.013577\n",
            "live           0.013453\n",
            "province       0.012905\n",
            "first          0.012837\n",
            "city           0.012793\n",
            "death          0.012317\n",
            "year           0.012240\n",
            "world          0.011623\n",
            "epidemic       0.011616\n",
            "amid           0.011082\n",
            "medical        0.011039\n",
            "president      0.010959\n",
            "dtype: float64\n",
            "\n",
            "USA:\n",
            "coronavirus    0.049972\n",
            "covid          0.048809\n",
            "china          0.041930\n",
            "vaccine        0.031179\n",
            "case           0.028555\n",
            "say            0.027612\n",
            "new            0.026551\n",
            "report         0.017860\n",
            "death          0.017719\n",
            "outbreak       0.017477\n",
            "virus          0.016571\n",
            "test           0.016000\n",
            "spread         0.014537\n",
            "people         0.013405\n",
            "health         0.013065\n",
            "trump          0.012863\n",
            "first          0.012383\n",
            "late           0.012190\n",
            "president      0.011938\n",
            "day            0.011102\n",
            "million        0.010978\n",
            "state          0.010971\n",
            "rise           0.010859\n",
            "country        0.010299\n",
            "world          0.009807\n",
            "positive       0.009792\n",
            "pandemic       0.009256\n",
            "infection      0.009202\n",
            "hit            0.008676\n",
            "confirm        0.008295\n",
            "dtype: float64\n",
            "\n",
            "China&USA:\n",
            "china          0.045033\n",
            "coronavirus    0.043029\n",
            "covid          0.042957\n",
            "case           0.027942\n",
            "vaccine        0.027795\n",
            "new            0.025423\n",
            "say            0.024511\n",
            "outbreak       0.017291\n",
            "report         0.017268\n",
            "death          0.015774\n",
            "virus          0.013796\n",
            "people         0.013548\n",
            "test           0.013425\n",
            "health         0.013033\n",
            "first          0.012092\n",
            "novel          0.011916\n",
            "hospital       0.011775\n",
            "spread         0.011651\n",
            "day            0.011465\n",
            "president      0.011090\n",
            "country        0.011073\n",
            "late           0.010992\n",
            "million        0.010481\n",
            "trump          0.010241\n",
            "confirm        0.010155\n",
            "world          0.009889\n",
            "state          0.009267\n",
            "infection      0.009075\n",
            "city           0.009014\n",
            "patient        0.008786\n",
            "dtype: float64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reduced graph"
      ],
      "metadata": {
        "id": "4nkLA8k0LB7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# less frequent words:\n",
        "less_important_words_China = [key for key in list(threshold_reverse(pr_China,0.0003)[0])]\n",
        "\n",
        "less_important_words_USA = [key for key in list(threshold_reverse(pr_USA,0.0003)[0])]\n",
        "\n",
        "less_important_words_China_USA = [key for key in list(threshold_reverse(pr_China_USA,0.0003)[0])]"
      ],
      "metadata": {
        "id": "6ndwCL9oLpDA"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# discard less frequent words\n",
        "cleaned_mostimp_text_China = cleaned_mostfreq_text_China.copy()\n",
        "for txt in range(len(cleaned_mostimp_text_China)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostimp_text_China))\n",
        "  for word in less_important_words_China:\n",
        "    if word in cleaned_mostimp_text_China[txt].split():\n",
        "      cleaned_mostimp_text_China[txt] = cleaned_mostimp_text_China[txt].replace(word, '')\n",
        "      cleaned_mostimp_text_China[txt] = \" \".join(cleaned_mostimp_text_China[txt].split())\n",
        "\n",
        "cleaned_mostimp_text_USA = cleaned_mostfreq_text_USA.copy()\n",
        "for txt in range(len(cleaned_mostimp_text_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostimp_text_USA))\n",
        "  for word in less_important_words_USA:\n",
        "    if word in cleaned_mostimp_text_USA[txt].split():\n",
        "      cleaned_mostimp_text_USA[txt] = cleaned_mostimp_text_USA[txt].replace(word, '')\n",
        "      cleaned_mostimp_text_USA[txt] = \" \".join(cleaned_mostimp_text_USA[txt].split())\n",
        "\n",
        "cleaned_mostimp_text_China_USA = cleaned_mostfreq_text_China_USA.copy()\n",
        "for txt in range(len(cleaned_mostimp_text_China_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostimp_text_China_USA))\n",
        "  for word in less_important_words_China_USA:\n",
        "    if word in cleaned_mostimp_text_China_USA[txt].split():\n",
        "      cleaned_mostimp_text_China_USA[txt] = cleaned_mostimp_text_China_USA[txt].replace(word, '')\n",
        "      cleaned_mostimp_text_China_USA[txt] = \" \".join(cleaned_mostimp_text_China_USA[txt].split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "814c0bc3-4c13-499e-f1a4-b09702d99f40",
        "id": "D8L4ETxALpDB"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 1336\n",
            "1000 / 1336\n",
            "0 / 3957\n",
            "1000 / 3957\n",
            "2000 / 3957\n",
            "3000 / 3957\n",
            "0 / 5293\n",
            "1000 / 5293\n",
            "2000 / 5293\n",
            "3000 / 5293\n",
            "4000 / 5293\n",
            "5000 / 5293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_mostimp_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_mostimp_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_mostimp_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0069fa8d-2275-4ca0-fb25-5666b4d9754d",
        "id": "X2ObCNtJLpDB"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  556\n",
            "USA:  686\n",
            "China&USA:  677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_China = pd.DataFrame.from_dict(freq_dict_China, orient='index').reset_index()\n",
        "df_China.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China.reset_index(inplace=True)\n",
        "df_China.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_USA = pd.DataFrame.from_dict(freq_dict_USA, orient='index').reset_index()\n",
        "df_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_USA.reset_index(inplace=True)\n",
        "df_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_China_USA = pd.DataFrame.from_dict(freq_dict_China_USA, orient='index').reset_index()\n",
        "df_China_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China_USA.reset_index(inplace=True)\n",
        "df_China_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "print('China')\n",
        "print(df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA')\n",
        "print(df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA')\n",
        "print(df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc05e37-69c1-4a40-e33d-2e45208dc5d7",
        "id": "DfrViAqzNnb9"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "           Word  Count\n",
            "0         covid    959\n",
            "1       vaccine    766\n",
            "2         china    328\n",
            "3           say    190\n",
            "4        report    175\n",
            "5        health    173\n",
            "6          case    172\n",
            "7       country    172\n",
            "8       receive    159\n",
            "9         first    156\n",
            "10      million    151\n",
            "11          new    150\n",
            "12          dos    123\n",
            "13       people    120\n",
            "14     pandemic    113\n",
            "15        batch    105\n",
            "16        world     96\n",
            "17    president     77\n",
            "18       accord     72\n",
            "19         dose     71\n",
            "20          use     70\n",
            "21     national     67\n",
            "22        death     65\n",
            "23          day     65\n",
            "24  vaccination     65\n",
            "25          one     64\n",
            "26         year     63\n",
            "27       brazil     63\n",
            "28   administer     59\n",
            "29       second     59\n",
            "\n",
            "USA\n",
            "           Word  Count\n",
            "0         covid   3021\n",
            "1       vaccine   2128\n",
            "2           say    878\n",
            "3           new    413\n",
            "4          case    395\n",
            "5       johnson    298\n",
            "6          shot    259\n",
            "7       million    258\n",
            "8        report    253\n",
            "9           use    253\n",
            "10       people    247\n",
            "11          get    247\n",
            "12      country    246\n",
            "13        india    238\n",
            "14       health    235\n",
            "15        first    235\n",
            "16          dos    220\n",
            "17        blood    217\n",
            "18  vaccination    216\n",
            "19         clot    215\n",
            "20     pandemic    213\n",
            "21        death    211\n",
            "22  coronavirus    210\n",
            "23        state    207\n",
            "24         year    177\n",
            "25       brazil    167\n",
            "26        surge    157\n",
            "27    president    148\n",
            "28        world    143\n",
            "29         risk    141\n",
            "\n",
            "China&USA\n",
            "           Word  Count\n",
            "0         covid   3980\n",
            "1       vaccine   2894\n",
            "2           say   1068\n",
            "3          case    567\n",
            "4           new    563\n",
            "5         china    460\n",
            "6        report    428\n",
            "7       country    418\n",
            "8       million    409\n",
            "9        health    408\n",
            "10        first    391\n",
            "11       people    367\n",
            "12      johnson    346\n",
            "13          dos    343\n",
            "14     pandemic    326\n",
            "15          use    323\n",
            "16         shot    290\n",
            "17      receive    286\n",
            "18          get    285\n",
            "19  vaccination    281\n",
            "20        death    276\n",
            "21        state    262\n",
            "22        blood    258\n",
            "23  coronavirus    253\n",
            "24        india    249\n",
            "25         clot    247\n",
            "26         year    240\n",
            "27        world    239\n",
            "28       brazil    230\n",
            "29    president    225\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_China = freq_dict_China.keys()  \n",
        "keys_USA = freq_dict_USA.keys()  \n",
        "keys_China_USA = freq_dict_China_USA.keys()  "
      ],
      "metadata": {
        "id": "Q52SCkkKNncF"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_China, network_df_China = create_network(cleaned_mostimp_text_China)\n",
        "network_USA, network_df_USA = create_network(cleaned_mostimp_text_USA)\n",
        "network_China_USA, network_df_China_USA = create_network(cleaned_mostimp_text_China_USA)"
      ],
      "metadata": {
        "id": "bPn3tfv7NncF"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(network_df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(network_df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(network_df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7850dc38-a528-4719-dfb5-6f8b27659c18",
        "id": "MVggBElVNncF"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "                    weight\n",
            "(covid, vaccine)       528\n",
            "(vaccine, china)       253\n",
            "(china, covid)         249\n",
            "(receive, vaccine)     171\n",
            "(covid, case)          143\n",
            "(million, covid)       141\n",
            "(health, covid)        139\n",
            "(vaccine, say)         136\n",
            "(vaccine, dos)         135\n",
            "(covid, say)           134\n",
            "(country, vaccine)     134\n",
            "(covid, country)       127\n",
            "(first, vaccine)       125\n",
            "(new, covid)           125\n",
            "(covid, report)        121\n",
            "(first, covid)         119\n",
            "(receive, covid)       118\n",
            "(million, vaccine)     113\n",
            "(vaccine, health)      110\n",
            "(batch, vaccine)       109\n",
            "(covid, dos)           105\n",
            "(covid, pandemic)      103\n",
            "(new, case)             98\n",
            "(covid, people)         92\n",
            "(batch, covid)          90\n",
            "(million, dos)          90\n",
            "(vaccine, use)          82\n",
            "(case, report)          81\n",
            "(world, covid)          74\n",
            "(dose, vaccine)         73\n",
            "\n",
            "USA:\n",
            "                      weight\n",
            "(vaccine, covid)        1459\n",
            "(covid, say)             649\n",
            "(vaccine, say)           633\n",
            "(new, covid)             363\n",
            "(covid, case)            354\n",
            "(johnson, vaccine)       287\n",
            "(johnson, covid)         239\n",
            "(use, vaccine)           238\n",
            "(vaccine, clot)          234\n",
            "(vaccine, blood)         233\n",
            "(vaccine, dos)           224\n",
            "(report, covid)          223\n",
            "(million, covid)         198\n",
            "(blood, clot)            197\n",
            "(country, covid)         195\n",
            "(covid, death)           195\n",
            "(covid, shot)            193\n",
            "(covid, first)           192\n",
            "(covid, get)             185\n",
            "(india, covid)           185\n",
            "(million, vaccine)       184\n",
            "(use, covid)             183\n",
            "(covid, health)          182\n",
            "(covid, dos)             174\n",
            "(people, covid)          165\n",
            "(shot, vaccine)          163\n",
            "(covid, vaccination)     162\n",
            "(state, covid)           162\n",
            "(get, vaccine)           160\n",
            "(country, vaccine)       152\n",
            "\n",
            "China&USA:\n",
            "                    weight\n",
            "(covid, vaccine)      1987\n",
            "(covid, say)           783\n",
            "(vaccine, say)         769\n",
            "(covid, case)          497\n",
            "(new, covid)           488\n",
            "(china, covid)         363\n",
            "(vaccine, dos)         359\n",
            "(covid, report)        344\n",
            "(johnson, vaccine)     343\n",
            "(million, covid)       339\n",
            "(vaccine, china)       325\n",
            "(covid, country)       322\n",
            "(health, covid)        321\n",
            "(vaccine, use)         320\n",
            "(first, covid)         311\n",
            "(million, vaccine)     297\n",
            "(country, vaccine)     286\n",
            "(receive, vaccine)     280\n",
            "(covid, dos)           279\n",
            "(vaccine, blood)       276\n",
            "(vaccine, clot)        268\n",
            "(johnson, covid)       257\n",
            "(covid, people)        257\n",
            "(covid, death)         256\n",
            "(first, vaccine)       251\n",
            "(covid, pandemic)      250\n",
            "(vaccine, health)      249\n",
            "(use, covid)           234\n",
            "(receive, covid)       233\n",
            "(blood, clot)          233\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph(network):\n",
        "  up_weighted = []\n",
        "  for edge in network:\n",
        "      #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
        "      up_weighted.append((edge[0],edge[1],network[edge]))\n",
        "      \n",
        "  #print(network)\n",
        "  #print(up_weighted[0:10])\n",
        "  G = nx.Graph()\n",
        "  G.add_weighted_edges_from(up_weighted)\n",
        "  return G"
      ],
      "metadata": {
        "id": "fmQTFAb3NncF"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_China = get_graph(network_China)\n",
        "G_USA = get_graph(network_USA)\n",
        "G_China_USA = get_graph(network_China_USA)"
      ],
      "metadata": {
        "id": "zNrH5boVNncF"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print('Nodes: ',len(G_China.nodes()))\n",
        "print('Edges: ',len(G_China.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China))\n",
        "print()\n",
        "print('USA:')\n",
        "print('Nodes: ',len(G_USA.nodes()))\n",
        "print('Edges: ',len(G_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_USA))\n",
        "print()\n",
        "print('China&USA:')\n",
        "print('Nodes: ',len(G_China_USA.nodes()))\n",
        "print('Edges: ',len(G_China_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China_USA))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ede46e2-a322-44c0-b7fa-d8cb55dca2cf",
        "id": "kx5041hFNncF"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "Nodes:  556\n",
            "Edges:  19649\n",
            "Is connected:  True\n",
            "\n",
            "USA:\n",
            "Nodes:  686\n",
            "Edges:  43779\n",
            "Is connected:  True\n",
            "\n",
            "China&USA:\n",
            "Nodes:  677\n",
            "Edges:  52248\n",
            "Is connected:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save edge list"
      ],
      "metadata": {
        "id": "p4SvWlzSfzFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = './edgelist_China_3_700.csv'\n",
        "nx.write_weighted_edgelist(G_China, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_China_3_700.csv\n",
        "files.download(\"edgelist_China_3_700.csv\")\n",
        "\n",
        "filename = './edgelist_USA_3_700.csv'\n",
        "nx.write_weighted_edgelist(G_USA, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_USA_3_700.csv\n",
        "files.download(\"edgelist_USA_3_700.csv\")\n",
        "\n",
        "filename = './edgelist_China_USA_3_700.csv'\n",
        "nx.write_weighted_edgelist(G_China_USA, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_China_USA_3_700.csv\n",
        "files.download(\"edgelist_China_USA_3_700.csv\")"
      ],
      "metadata": {
        "id": "zgmmRGRCrM9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "74feef77-a1ac-47b5-e7e9-4d2c010b4664"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9208ec2d-b742-47d2-812e-5b1ea114548b\", \"edgelist_China_3_700.csv\", 310664)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6f5267e3-473d-4ca8-8bbb-abfe1e6c149a\", \"edgelist_USA_3_700.csv\", 679085)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_fb2c2d36-6b7a-4718-bf26-1a23bd78b5e8\", \"edgelist_China_USA_3_700.csv\", 813319)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create Node List\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nWHLO7AhdwzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nodes(freq_dict, name):\n",
        "  word_nodes = pd.DataFrame.from_dict(freq_dict,orient=\"index\")\n",
        "  word_nodes.reset_index(inplace=True)\n",
        "  word_nodes[\"Label\"] = word_nodes[\"index\"]\n",
        "  word_nodes.rename(columns={\"index\":\"Id\",0:\"delete\"},inplace=True)\n",
        "  word_nodes = word_nodes.drop(columns=['delete'])\n",
        "  nodelist = pd.DataFrame()\n",
        "  nodelist = nodelist.append(word_nodes, ignore_index=True)\n",
        "\n",
        "  nodelist = nodelist.to_csv(\"nodelist_\"+name+\".csv\",index=False)\n",
        "  files.download(\"nodelist_\"+name+\".csv\")\n",
        "  return nodelist, word_nodes"
      ],
      "metadata": {
        "id": "v2GYb2BQFzET"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodelist_China, word_nodes_China = nodes(freq_dict_China,\"China_3_700\")\n",
        "nodelist_USA, word_nodes_USA = nodes(freq_dict_USA,\"USA_3_700\")\n",
        "nodelist_China_USA, word_nodes_China_USA = nodes(freq_dict_China_USA,\"China_3_USA_700\")\n",
        "\n",
        "print('China:')\n",
        "print(word_nodes_China.head())\n",
        "print()\n",
        "print('USA:')\n",
        "print(word_nodes_USA.head())\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(word_nodes_China_USA.head())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "EmQrEHV0F1QY",
        "outputId": "c8a619a3-af58-474d-c534-af78c9e4c4eb"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5ea47f59-cfa4-497c-8b01-1b77d46eb8dd\", \"nodelist_China_3_700.csv\", 7859)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_dae59bb9-8358-4432-9b83-a24b5121672d\", \"nodelist_USA_3_700.csv\", 9509)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_86d08d78-d45d-4bcf-9c78-d8228f75af17\", \"nodelist_China_3_USA_700.csv\", 9373)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "        Id    Label\n",
            "0    covid    covid\n",
            "1  vaccine  vaccine\n",
            "2    china    china\n",
            "3      say      say\n",
            "4   report   report\n",
            "\n",
            "USA:\n",
            "        Id    Label\n",
            "0    covid    covid\n",
            "1  vaccine  vaccine\n",
            "2      say      say\n",
            "3      new      new\n",
            "4     case     case\n",
            "\n",
            "China&USA:\n",
            "        Id    Label\n",
            "0    covid    covid\n",
            "1  vaccine  vaccine\n",
            "2      say      say\n",
            "3     case     case\n",
            "4      new      new\n",
            "\n"
          ]
        }
      ]
    }
  ]
}