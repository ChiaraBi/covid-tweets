{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summary_period1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#notebook to download the csv of edges and nodes of a given network\n",
        "import os\n",
        "import requests \n",
        "import time\n",
        "import string\n",
        "import networkx as nx\n",
        "import itertools\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.corpus import wordnet as wn #importing it\n",
        "from nltk.stem.wordnet import WordNetLemmatizer #importing wordnet lemmatizer\n",
        "from nltk import pos_tag #part-of-speech-tagger\n",
        "from collections import defaultdict #defaultdict returns default value for non-existant keys you try to  access based on the function you passed in the constructor\n",
        "from google.colab import files\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRcCylkZqDPi",
        "outputId": "1f276ac2-d067-4422-98b2-a24dac9c11b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(df):       #extract the text from the tweets and RT\n",
        "                            #works ONLY on .csv file\n",
        "  list_strings = []\n",
        "  for index in range(len(df)):\n",
        "    if index % 1000 == 0:\n",
        "      print(str(index)+' / '+str(len(df)))\n",
        "    text = df.loc[index]['text']                          #if it is nor trucated nor a RT  i take \"text\"\n",
        "    string = -1\n",
        "    if (df.loc[index,\"truncated\"] == True):                 #if it is trucated I take \"extended_tweet\"\n",
        "        string = df.loc[index,\"extended_tweet\"]\n",
        "    if type(df.loc[index,\"retweeted_status\"]) != float:     #if it is a RT I take retweeted_status\n",
        "        string = df.loc[index,\"retweeted_status\"]\n",
        "    if type(string) == str :\n",
        "        if(re.search('full_text\\':(.+?)https',string) != None):     #if I find \"full_text\"\n",
        "          s = re.search('full_text\\':(.+?)https',string).group(1)\n",
        "        if(re.search('text\\':(.+?)https',string)!= None):\n",
        "          s = re.search('text\\':(.+?)https',string).group(1)\n",
        "        else: \n",
        "          continue\n",
        "        list_strings.append(s)\n",
        "        #print(s)         \n",
        "    else:\n",
        "      list_strings.append(text)\n",
        "      #print(text)\n",
        "      \n",
        "\n",
        "  return list_strings"
      ],
      "metadata": {
        "id": "CGa_FVVGqDSJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning, lemmatising and pos tagging tweets\n",
        "\n",
        "nltk.download('words')\n",
        "WORDS = set(nltk.corpus.words.words()) #the last two lines serve to download the corpus of standard English language words\n",
        "nltk.download('stopwords') #downloading stopwords\n",
        "STOP_WORDS = set(nltk.corpus.stopwords.words(\"english\")) #taking the stop words from English language\n",
        "nltk.download('wordnet') #downloading wordnet\n",
        "nltk.download('averaged_perceptron_tagger') #downloading tagger\n",
        "tag_map = defaultdict(lambda : wn.NOUN) #here we define that wn.NOUN is the default value for the dict\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "def lemma_pos_cleaner(tweet):\n",
        "\n",
        "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) # remove mentions\n",
        "    tweet = re.sub(\"#[A-Za-z0-9]+\", \"\",tweet) # remove hashtags\n",
        "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) # remove http links\n",
        "    tweet = \" \".join(tweet.split())\n",
        "    tweet = str.lower(tweet) #to lowercase \n",
        "    tweet = re.sub(\"'\",\" \",tweet) # remove aphostrophe\n",
        "\n",
        "    #basically we use pos_tag function on tokens that we get by applying wordpunct tokenization\n",
        "    #to tweet (it separates all the words and symbols)\n",
        "    #then we pass the token along with it's wordnet pos value that we get from the tag_map dictionary (noun, adjective, verb or adverb) to the lemma function (the WordNetLemmatizer())\n",
        "    lemma_function = WordNetLemmatizer()\n",
        "    tweet = \" \".join(lemma_function.lemmatize(token, tag_map[tag[0]]) for token, tag in nltk.pos_tag(nltk.wordpunct_tokenize(tweet))) #lemmatize\n",
        "  \n",
        "\n",
        "    # francesco: I removed also all 2 letters words and added specific words, words that appears frequently but are discarded because they are not in the english language\n",
        "    SPECIFIC_WORDS = ['virus', 'coronavirus', 'covid19', 'covid', 'trump', 'hubei', 'beijing', 'xinjiang', 'jinping', 'korea', 'xinhua', 'india', 'taiwan','johnson','singapore', 'africa', 'japanese', 'france', 'asian', 'australia', 'french', 'asia', 'leishenshan', 'british', 'qingdao', 'fauci', 'america',  'california', 'sichuan', 'malaysia', 'huawei','thailand', 'shandong', 'italy', 'philippines', 'germany', 'facebook', 'african', 'shenzhen', 'tokyo', 'russian','uygur', '5g', 'pompeo', 'vietnam', 'australian', 'cambodia', 'zhejiang', 'yunnan', 'guangdong', 'korean', 'iran', 'washington']\n",
        "    tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) if (w in WORDS or w in SPECIFIC_WORDS) and len(w)>2 and w not in STOP_WORDS ) #remove stop words\n",
        "   \n",
        "    return tweet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD0LDoYVqDUh",
        "outputId": "8b6ff527-7e43-426f-c9f1-d78ad7b5bb26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def frequency_dictionary(df):\n",
        "  unique_words = {}\n",
        "\n",
        "  for row in df:\n",
        "    for word in row.split():\n",
        "      #if the word is encountered for the first time add to dict as key and set its value to 0\n",
        "      unique_words.setdefault(word,0)\n",
        "      #increase the value (i.e the count) of the word by 1 every time it is encountered\n",
        "      unique_words[word] += 1\n",
        "\n",
        "  return unique_words"
      ],
      "metadata": {
        "id": "MAfOr8vR4P88"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "period = '_JanFeb2020'  # _JanFeb2020, _MarchApril2021, _SeptOct2020"
      ],
      "metadata": {
        "id": "o-GUlzAVOwk_"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "China = pd.read_csv('/content/China'+period+'.csv')\n",
        "USA = pd.read_csv('/content/USA'+period+'.csv')\n",
        "China_USA = pd.read_csv('/content/China&USA'+period+'.csv')"
      ],
      "metadata": {
        "id": "suPdLaUx1Ct8"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of tweets:\n",
        "print('China: ', len(China))\n",
        "print('USA: ', len(USA))\n",
        "print('China&USA: ', len(China_USA))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeO9nxO6Br3x",
        "outputId": "8e833cfd-8e0c-4622-92ca-ef2b64853378"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  4876\n",
            "USA:  7519\n",
            "China&USA:  12395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_China = extract_text(China)\n",
        "text_USA = extract_text(USA)\n",
        "text_China_USA = extract_text(China_USA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WI3Rz9T1Cxo",
        "outputId": "b467fb44-a320-49a2-96d5-6d8af042b507"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 4876\n",
            "1000 / 4876\n",
            "2000 / 4876\n",
            "3000 / 4876\n",
            "4000 / 4876\n",
            "0 / 7519\n",
            "1000 / 7519\n",
            "2000 / 7519\n",
            "3000 / 7519\n",
            "4000 / 7519\n",
            "5000 / 7519\n",
            "6000 / 7519\n",
            "7000 / 7519\n",
            "0 / 12395\n",
            "1000 / 12395\n",
            "2000 / 12395\n",
            "3000 / 12395\n",
            "4000 / 12395\n",
            "5000 / 12395\n",
            "6000 / 12395\n",
            "7000 / 12395\n",
            "8000 / 12395\n",
            "9000 / 12395\n",
            "10000 / 12395\n",
            "11000 / 12395\n",
            "12000 / 12395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text_China = [lemma_pos_cleaner(txt) for txt in text_China]\n",
        "cleaned_text_USA = [lemma_pos_cleaner(txt) for txt in text_USA]\n",
        "cleaned_text_China_USA = [lemma_pos_cleaner(txt) for txt in text_China_USA]\n",
        "\n",
        "print('China:')\n",
        "print(cleaned_text_China[0:10])\n",
        "print()\n",
        "print('USA:')\n",
        "print(cleaned_text_USA[0:10])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(cleaned_text_China_USA[0:10])"
      ],
      "metadata": {
        "id": "6q4JI_jV1C2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4542cdf-4445-4d5c-9673-8a17615a1a64"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "['talk university challenge china face deal misconception people medium watch', 'spring always bring along lively saturate sea colorful bloom flower look collection flower blossom across china enjoy upcoming new season', 'mean carnival brazil year one samba school take inspiration china', 'china world make great stride track infectious disease take unexpected turn make hard track', 'fake news story case covid lead protest evacuation people', 'transmission outside china graphic', 'world health organization announce lead team international expert currently china', 'outside china confirm case novel covid half cruise ship dock japan rest scatter among country mostly asia', 'five cargo aircraft airway head city beijing shanghai carry ton medical supply gratis include million mask hand send epidemic prevention supply purchase china government', 'case miss jinping chair lead body meeting covid control']\n",
            "\n",
            "USA:\n",
            "['shameful attack bus carry evacuee china coronavirus outbreak', 'china hubei say people province prison system also infect coronavirus', 'china coronavirus outbreak late update', 'china foreign journalist coronavirus death climb', 'china coronavirus outbreak late update', 'least people china die spread country', 'aid agency warn patient china run drug amid coronavirus', 'china revoke press credential three wall street journal coronavirus opinion piece deem racist late', 'least people china report recover coronavirus survivor recall ordeal', 'concern china mass surveillance system use combat spread coronavirus']\n",
            "\n",
            "China&USA:\n",
            "['talk university challenge china face deal misconception people medium watch', 'spring always bring along lively saturate sea colorful bloom flower look collection flower blossom across china enjoy upcoming new season', 'mean carnival brazil year one samba school take inspiration china', 'china world make great stride track infectious disease take unexpected turn make hard track', 'fake news story case covid lead protest evacuation people', 'transmission outside china graphic', 'world health organization announce lead team international expert currently china', 'outside china confirm case novel covid half cruise ship dock japan rest scatter among country mostly asia', 'five cargo aircraft airway head city beijing shanghai carry ton medical supply gratis include million mask hand send epidemic prevention supply purchase china government', 'case miss jinping chair lead body meeting covid control']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCiw0q7t4UER",
        "outputId": "8f877e2d-b753-4519-dbd1-4a48032f7ab2"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  3617\n",
            "USA:  3380\n",
            "China&USA:  4883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most frequent words\n",
        "print('China')\n",
        "print([key for key in freq_dict_China.keys() if freq_dict_China[key]>200])\n",
        "print()\n",
        "print('USA')\n",
        "print([key for key in freq_dict_USA.keys() if freq_dict_USA[key]>400])\n",
        "print()\n",
        "print('China&USA')\n",
        "print([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]>600])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y29OU9Cd5Auv",
        "outputId": "466fd183-bfed-4f0b-b3b1-1eb0e021c4a7"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "['china', 'novel', 'hospital', 'coronavirus', 'case', 'outbreak', 'new', 'patient', 'fight', 'say', 'confirm', 'people', 'epidemic', 'medical', 'hubei', 'province', 'battle', 'support', 'report', 'health', 'live', 'city', 'amid', 'death', 'day', 'control', 'virus', 'year', 'take', 'effort', 'country', 'help', 'one', 'discharge', 'work']\n",
            "\n",
            "USA\n",
            "['coronavirus', 'china', 'outbreak', 'virus', 'case', 'say', 'new', 'spread', 'death', 'people', 'health', 'late', 'report', 'cruise', 'first', 'ship']\n",
            "\n",
            "China&USA\n",
            "['china', 'coronavirus', 'outbreak', 'case', 'new', 'novel', 'say', 'virus', 'hospital', 'people', 'death', 'spread', 'health', 'patient', 'confirm', 'report', 'fight', 'city', 'late']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# less frequent words"
      ],
      "metadata": {
        "id": "9oHPXYbpINWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# less frequent words:\n",
        "print('Less frequent China: ', len([key for key in freq_dict_China.keys() if freq_dict_China[key]<5]))\n",
        "print('More frequent China: ', len([key for key in freq_dict_China.keys() if freq_dict_China[key]>=5]))\n",
        "less_frequent_words_China = [key for key in freq_dict_China.keys() if freq_dict_China[key]<5]\n",
        "\n",
        "print('Less frequent USA: ', len([key for key in freq_dict_USA.keys() if freq_dict_USA[key]<5]))\n",
        "print('More frequent USA:', len([key for key in freq_dict_USA.keys() if freq_dict_USA[key]>=5]))\n",
        "less_frequent_words_USA = [key for key in freq_dict_USA.keys() if freq_dict_USA[key]<5]\n",
        "\n",
        "print('Less frequent China&USA: ', len([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]<5]))\n",
        "print('More frequent China&USA: ', len([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]>=5]))\n",
        "less_frequent_words_China_USA = [key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]<5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgiQzaHD5zAb",
        "outputId": "0a59dea1-a725-4f1f-e6ab-8ce9f7492e47"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Less frequent China:  2132\n",
            "More frequent China:  1485\n",
            "Less frequent USA:  1709\n",
            "More frequent USA: 1671\n",
            "Less frequent China&USA:  2513\n",
            "More frequent China&USA:  2370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# discard less frequent words\n",
        "cleaned_mostfreq_text_China = cleaned_text_China.copy()\n",
        "for txt in range(len(cleaned_mostfreq_text_China)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostfreq_text_China))\n",
        "  for word in less_frequent_words_China:\n",
        "    if word in cleaned_mostfreq_text_China[txt].split():\n",
        "      cleaned_mostfreq_text_China[txt] = cleaned_mostfreq_text_China[txt].replace(word, '')\n",
        "      cleaned_mostfreq_text_China[txt] = \" \".join(cleaned_mostfreq_text_China[txt].split())\n",
        "\n",
        "cleaned_mostfreq_text_USA = cleaned_text_USA.copy()\n",
        "for txt in range(len(cleaned_mostfreq_text_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostfreq_text_USA))\n",
        "  for word in less_frequent_words_USA:\n",
        "    if word in cleaned_mostfreq_text_USA[txt].split():\n",
        "      cleaned_mostfreq_text_USA[txt] = cleaned_mostfreq_text_USA[txt].replace(word, '')\n",
        "      cleaned_mostfreq_text_USA[txt] = \" \".join(cleaned_mostfreq_text_USA[txt].split())\n",
        "\n",
        "cleaned_mostfreq_text_China_USA = cleaned_text_China_USA.copy()\n",
        "for txt in range(len(cleaned_mostfreq_text_China_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostfreq_text_China_USA))\n",
        "  for word in less_frequent_words_China_USA:\n",
        "    if word in cleaned_mostfreq_text_China_USA[txt].split():\n",
        "      cleaned_mostfreq_text_China_USA[txt] = cleaned_mostfreq_text_China_USA[txt].replace(word, '')\n",
        "      cleaned_mostfreq_text_China_USA[txt] = \" \".join(cleaned_mostfreq_text_China_USA[txt].split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmlqtzHp6lh8",
        "outputId": "27173941-aced-42ef-a92e-460eaf2ca4be"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 4843\n",
            "1000 / 4843\n",
            "2000 / 4843\n",
            "3000 / 4843\n",
            "4000 / 4843\n",
            "0 / 7519\n",
            "1000 / 7519\n",
            "2000 / 7519\n",
            "3000 / 7519\n",
            "4000 / 7519\n",
            "5000 / 7519\n",
            "6000 / 7519\n",
            "7000 / 7519\n",
            "0 / 12362\n",
            "1000 / 12362\n",
            "2000 / 12362\n",
            "3000 / 12362\n",
            "4000 / 12362\n",
            "5000 / 12362\n",
            "6000 / 12362\n",
            "7000 / 12362\n",
            "8000 / 12362\n",
            "9000 / 12362\n",
            "10000 / 12362\n",
            "11000 / 12362\n",
            "12000 / 12362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_mostfreq_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_mostfreq_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_mostfreq_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KHj5-o57f8y",
        "outputId": "adc917e4-6929-4720-d858-1cf88aefb8a8"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  1492\n",
            "USA:  1673\n",
            "China&USA:  2375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Network"
      ],
      "metadata": {
        "id": "1davLfA3HNV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_China = pd.DataFrame.from_dict(freq_dict_China, orient='index').reset_index()\n",
        "df_China.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China.reset_index(inplace=True)\n",
        "df_China.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_USA = pd.DataFrame.from_dict(freq_dict_USA, orient='index').reset_index()\n",
        "df_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_USA.reset_index(inplace=True)\n",
        "df_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_China_USA = pd.DataFrame.from_dict(freq_dict_China_USA, orient='index').reset_index()\n",
        "df_China_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China_USA.reset_index(inplace=True)\n",
        "df_China_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "print('China')\n",
        "print(df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA')\n",
        "print(df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA')\n",
        "print(df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxPPnevj890b",
        "outputId": "3f83edd9-8750-4d95-c490-87db9d894d45"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "           Word  Count\n",
            "0         china   3018\n",
            "1         novel   1385\n",
            "2      hospital    969\n",
            "3   coronavirus    949\n",
            "4          case    863\n",
            "5      outbreak    846\n",
            "6           new    657\n",
            "7       patient    651\n",
            "8         fight    583\n",
            "9           say    521\n",
            "10      confirm    454\n",
            "11       people    426\n",
            "12     epidemic    403\n",
            "13      medical    378\n",
            "14        hubei    369\n",
            "15     province    355\n",
            "16       battle    311\n",
            "17      support    310\n",
            "18       report    299\n",
            "19       health    294\n",
            "20         live    294\n",
            "21         city    293\n",
            "22         amid    289\n",
            "23        death    288\n",
            "24          day    273\n",
            "25      control    266\n",
            "26        virus    263\n",
            "27         year    233\n",
            "28         take    223\n",
            "29       effort    213\n",
            "\n",
            "USA\n",
            "           Word  Count\n",
            "0   coronavirus   5134\n",
            "1         china   3946\n",
            "2      outbreak   1404\n",
            "3         virus   1133\n",
            "4          case   1110\n",
            "5           say    933\n",
            "6           new    906\n",
            "7        spread    858\n",
            "8         death    734\n",
            "9        people    624\n",
            "10       health    603\n",
            "11         late    459\n",
            "12       cruise    414\n",
            "13       report    414\n",
            "14        first    413\n",
            "15         ship    408\n",
            "16         city    394\n",
            "17      confirm    392\n",
            "18         toll    374\n",
            "19        world    370\n",
            "20   quarantine    369\n",
            "21        japan    366\n",
            "22        state    362\n",
            "23      country    348\n",
            "24       travel    348\n",
            "25       flight    335\n",
            "26         fear    329\n",
            "27          hit    321\n",
            "28         test    310\n",
            "29         rise    304\n",
            "\n",
            "China&USA\n",
            "           Word  Count\n",
            "0         china   6964\n",
            "1   coronavirus   6084\n",
            "2      outbreak   2250\n",
            "3          case   1973\n",
            "4           new   1563\n",
            "5         novel   1539\n",
            "6           say   1454\n",
            "7         virus   1395\n",
            "8      hospital   1177\n",
            "9        people   1050\n",
            "10        death   1022\n",
            "11       spread   1013\n",
            "12       health    897\n",
            "13      patient    851\n",
            "14      confirm    846\n",
            "15       report    713\n",
            "16        fight    707\n",
            "17         city    687\n",
            "18         late    638\n",
            "19        first    600\n",
            "20        hubei    588\n",
            "21          day    568\n",
            "22     epidemic    565\n",
            "23        world    562\n",
            "24      country    559\n",
            "25         amid    533\n",
            "26     province    525\n",
            "27      medical    495\n",
            "28     official    484\n",
            "29       infect    462\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_China = freq_dict_China.keys()  \n",
        "keys_USA = freq_dict_USA.keys()  \n",
        "keys_China_USA = freq_dict_China_USA.keys()  "
      ],
      "metadata": {
        "id": "zHmEpbLtr46D"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_network(cleaned_text):\n",
        "  network = {}\n",
        "  #connect the word that appear in the same tweets\n",
        "  for row in cleaned_text:\n",
        "    combined_list = [word for word in str.split(row)]\n",
        "    #for pair in itertools.product(combined_list, combined_list):\n",
        "    #print(combined_list)\n",
        "    for pair in itertools.product(combined_list, combined_list):\n",
        "          #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
        "          if pair[0]!=pair[1] and not(pair[::-1] in network):\n",
        "              network.setdefault(pair,0)\n",
        "              network[pair] += 1 \n",
        "  network_df = pd.DataFrame.from_dict(network, orient=\"index\")\n",
        "  network_df.columns = [\"weight\"]\n",
        "  network_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
        "  return network, network_df"
      ],
      "metadata": {
        "id": "nUAWB6lT1DAd"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_China, network_df_China = create_network(cleaned_mostfreq_text_China)\n",
        "network_USA, network_df_USA = create_network(cleaned_mostfreq_text_USA)\n",
        "network_China_USA, network_df_China_USA = create_network(cleaned_mostfreq_text_China_USA)"
      ],
      "metadata": {
        "id": "7vR_r37vAGye"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(network_df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(network_df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(network_df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxMWDZua_b-l",
        "outputId": "034d059b-4290-4669-c091-be2c13fe55ab"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "                         weight\n",
            "(china, novel)             1024\n",
            "(china, outbreak)           644\n",
            "(china, coronavirus)        619\n",
            "(confirm, case)             571\n",
            "(patient, hospital)         560\n",
            "(china, case)               527\n",
            "(china, fight)              502\n",
            "(novel, coronavirus)        483\n",
            "(new, case)                 479\n",
            "(china, new)                427\n",
            "(china, say)                422\n",
            "(novel, outbreak)           394\n",
            "(case, report)              368\n",
            "(case, death)               357\n",
            "(epidemic, china)           346\n",
            "(china, support)            339\n",
            "(china, hospital)           334\n",
            "(china, confirm)            311\n",
            "(new, confirm)              296\n",
            "(china, people)             281\n",
            "(china, province)           272\n",
            "(china, battle)             266\n",
            "(china, hubei)              258\n",
            "(report, new)               245\n",
            "(new, death)                245\n",
            "(fight, novel)              233\n",
            "(makeshift, hospital)       233\n",
            "(china, effort)             232\n",
            "(novel, hospital)           231\n",
            "(coronavirus, outbreak)     227\n",
            "\n",
            "USA:\n",
            "                         weight\n",
            "(china, coronavirus)       2552\n",
            "(coronavirus, outbreak)    1118\n",
            "(china, outbreak)           937\n",
            "(coronavirus, case)         923\n",
            "(china, virus)              713\n",
            "(spread, coronavirus)       682\n",
            "(new, coronavirus)          642\n",
            "(china, case)               627\n",
            "(say, coronavirus)          605\n",
            "(china, death)              596\n",
            "(china, new)                561\n",
            "(coronavirus, death)        534\n",
            "(china, say)                521\n",
            "(china, spread)             504\n",
            "(coronavirus, health)       467\n",
            "(people, coronavirus)       463\n",
            "(china, people)             426\n",
            "(cruise, ship)              375\n",
            "(china, late)               367\n",
            "(confirm, case)             364\n",
            "(coronavirus, late)         363\n",
            "(china, health)             360\n",
            "(death, toll)               358\n",
            "(travel, china)             349\n",
            "(china, toll)               327\n",
            "(coronavirus, confirm)      326\n",
            "(coronavirus, cruise)       325\n",
            "(coronavirus, first)        318\n",
            "(report, coronavirus)       310\n",
            "(coronavirus, ship)         306\n",
            "\n",
            "China&USA:\n",
            "                         weight\n",
            "(china, coronavirus)       3171\n",
            "(china, outbreak)          1581\n",
            "(coronavirus, outbreak)    1346\n",
            "(china, case)              1154\n",
            "(coronavirus, case)        1146\n",
            "(china, novel)             1109\n",
            "(china, new)                988\n",
            "(china, say)                943\n",
            "(confirm, case)             935\n",
            "(china, virus)              914\n",
            "(coronavirus, new)          806\n",
            "(china, death)              804\n",
            "(new, case)                 783\n",
            "(spread, coronavirus)       724\n",
            "(china, people)             707\n",
            "(say, coronavirus)          682\n",
            "(novel, coronavirus)        638\n",
            "(case, death)               625\n",
            "(china, spread)             603\n",
            "(patient, hospital)         594\n",
            "(coronavirus, death)        585\n",
            "(china, fight)              558\n",
            "(health, china)             555\n",
            "(case, report)              543\n",
            "(coronavirus, people)       526\n",
            "(health, coronavirus)       513\n",
            "(china, confirm)            511\n",
            "(china, report)             491\n",
            "(china, late)               489\n",
            "(new, death)                457\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph\n"
      ],
      "metadata": {
        "id": "gfvo8x0Ku78D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph(network):\n",
        "  up_weighted = []\n",
        "  for edge in network:\n",
        "      #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
        "      up_weighted.append((edge[0],edge[1],network[edge]))\n",
        "      \n",
        "  #print(network)\n",
        "  #print(up_weighted[0:10])\n",
        "  G = nx.Graph()\n",
        "  G.add_weighted_edges_from(up_weighted)\n",
        "  return G"
      ],
      "metadata": {
        "id": "2NGKuSuYAo-K"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_China = get_graph(network_China)\n",
        "G_USA = get_graph(network_USA)\n",
        "G_China_USA = get_graph(network_China_USA)"
      ],
      "metadata": {
        "id": "TbpTF19bBKJq"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print('Nodes: ',len(G_China.nodes()))\n",
        "print('Edges: ',len(G_China.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China))\n",
        "print()\n",
        "print('USA:')\n",
        "print('Nodes: ',len(G_USA.nodes()))\n",
        "print('Edges: ',len(G_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_USA))\n",
        "print()\n",
        "print('China&USA:')\n",
        "print('Nodes: ',len(G_China_USA.nodes()))\n",
        "print('Edges: ',len(G_China_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China_USA))"
      ],
      "metadata": {
        "id": "W178ljb9rM6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442066b3-c4ab-4f36-c8f9-3ccfdafd302d"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "Nodes:  1492\n",
            "Edges:  82720\n",
            "Is connected:  True\n",
            "\n",
            "USA:\n",
            "Nodes:  1673\n",
            "Edges:  78390\n",
            "Is connected:  True\n",
            "\n",
            "China&USA:\n",
            "Nodes:  2375\n",
            "Edges:  158504\n",
            "Is connected:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PageRank"
      ],
      "metadata": {
        "id": "1LYNr4McLtWD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "P6qJVv9vHIr1"
      },
      "outputs": [],
      "source": [
        "# Calculating the pagerank on graph G, teleportation probability here is 0.15 but since the graph is strongly connected we can set it to zero if we want\n",
        "pr_China = nx.algorithms.pagerank(G_China,alpha = 1)\n",
        "pr_China = dict(sorted(pr_China.items(), key=lambda item: item[1],reverse  = True))\n",
        "\n",
        "pr_USA = nx.algorithms.pagerank(G_USA,alpha = 1)\n",
        "pr_USA = dict(sorted(pr_USA.items(), key=lambda item: item[1],reverse  = True))\n",
        "\n",
        "pr_China_USA = nx.algorithms.pagerank(G_China_USA,alpha = 1)\n",
        "pr_China_USA = dict(sorted(pr_China_USA.items(), key=lambda item: item[1],reverse  = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "BcVoAXxqpqLd"
      },
      "outputs": [],
      "source": [
        "def threshold(vector,threshold):\n",
        "\n",
        "  l = [(el,vector[el]) for el in vector if vector[el] >= threshold ]\n",
        "\n",
        "  return pd.DataFrame(l)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def threshold_reverse(vector,threshold):\n",
        "\n",
        "  l = [(el,vector[el]) for el in vector if vector[el] < threshold ]\n",
        "\n",
        "  return pd.DataFrame(l)"
      ],
      "metadata": {
        "id": "7-UFIOEJL6bY"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cUhquN72cI6",
        "outputId": "ac67a40f-1559-4765-b515-792454e3682a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  645\n",
            "\n",
            "first:\n",
            "              0         1\n",
            "0         china  0.052924\n",
            "1         novel  0.025584\n",
            "2      hospital  0.017164\n",
            "3   coronavirus  0.016114\n",
            "4          case  0.015881\n",
            "5      outbreak  0.015444\n",
            "6       patient  0.011923\n",
            "7           new  0.011853\n",
            "8           say  0.011254\n",
            "9         fight  0.010305\n",
            "10      confirm  0.009001\n",
            "11       people  0.008836\n",
            "12      medical  0.007752\n",
            "13     province  0.007708\n",
            "14     epidemic  0.007650\n",
            "15        hubei  0.007484\n",
            "16       health  0.006360\n",
            "17         city  0.006205\n",
            "18      support  0.006089\n",
            "19       report  0.005872\n",
            "20        death  0.005794\n",
            "21       battle  0.005586\n",
            "22          day  0.005476\n",
            "23        virus  0.005410\n",
            "24         year  0.004939\n",
            "25          one  0.004876\n",
            "26         amid  0.004870\n",
            "27      control  0.004748\n",
            "28         take  0.004553\n",
            "29      country  0.004448\n",
            "\n",
            "last:\n",
            "                0         1\n",
            "615          pass  0.000313\n",
            "616          suit  0.000312\n",
            "617           per  0.000312\n",
            "618       witness  0.000312\n",
            "619      standard  0.000311\n",
            "620  conversation  0.000311\n",
            "621        affect  0.000310\n",
            "622      separate  0.000310\n",
            "623       edition  0.000310\n",
            "624  disinfection  0.000310\n",
            "625          warn  0.000310\n",
            "626          tang  0.000308\n",
            "627     professor  0.000307\n",
            "628       history  0.000307\n",
            "629        tariff  0.000305\n",
            "630       publish  0.000305\n",
            "631        person  0.000305\n",
            "632       believe  0.000304\n",
            "633         right  0.000304\n",
            "634      multiple  0.000304\n",
            "635          type  0.000304\n",
            "636        highly  0.000304\n",
            "637        sector  0.000302\n",
            "638    difficulty  0.000302\n",
            "639     something  0.000302\n",
            "640       finally  0.000302\n",
            "641        handle  0.000302\n",
            "642           lam  0.000302\n",
            "643     gradually  0.000302\n",
            "644          army  0.000300\n",
            "\n",
            "USA:  602\n",
            "\n",
            "first:\n",
            "              0         1\n",
            "0   coronavirus  0.058512\n",
            "1         china  0.047678\n",
            "2      outbreak  0.020155\n",
            "3          case  0.014923\n",
            "4         virus  0.013855\n",
            "5           say  0.013385\n",
            "6           new  0.012331\n",
            "7        spread  0.011035\n",
            "8        health  0.010220\n",
            "9        people  0.010159\n",
            "10        death  0.009788\n",
            "11         city  0.006017\n",
            "12        world  0.005769\n",
            "13      country  0.005667\n",
            "14        state  0.005636\n",
            "15       travel  0.005629\n",
            "16        first  0.005581\n",
            "17       cruise  0.005491\n",
            "18         ship  0.005455\n",
            "19       report  0.005421\n",
            "20   quarantine  0.005129\n",
            "21         toll  0.005089\n",
            "22     official  0.005078\n",
            "23      confirm  0.005038\n",
            "24          day  0.004925\n",
            "25       global  0.004582\n",
            "26        japan  0.004428\n",
            "27          two  0.004177\n",
            "28         rise  0.004159\n",
            "29       infect  0.004116\n",
            "\n",
            "last:\n",
            "                  0         1\n",
            "572    nonessential  0.000328\n",
            "573            fund  0.000327\n",
            "574        diplomat  0.000327\n",
            "575          former  0.000325\n",
            "576       allegedly  0.000322\n",
            "577          inside  0.000320\n",
            "578          tariff  0.000320\n",
            "579        customer  0.000318\n",
            "580            team  0.000317\n",
            "581      technology  0.000317\n",
            "582            rein  0.000316\n",
            "583            cost  0.000316\n",
            "584        coverage  0.000314\n",
            "585          prompt  0.000312\n",
            "586            four  0.000311\n",
            "587             car  0.000310\n",
            "588            half  0.000309\n",
            "589        stricken  0.000309\n",
            "590    announcement  0.000309\n",
            "591       currently  0.000307\n",
            "592         founder  0.000307\n",
            "593         capital  0.000306\n",
            "594        attorney  0.000306\n",
            "595         arrival  0.000305\n",
            "596        transmit  0.000305\n",
            "597          policy  0.000304\n",
            "598            link  0.000304\n",
            "599           sweep  0.000304\n",
            "600          single  0.000301\n",
            "601  misinformation  0.000300\n",
            "\n",
            "China&USA:  617\n",
            "\n",
            "first:\n",
            "              0         1\n",
            "0         china  0.048848\n",
            "1   coronavirus  0.038912\n",
            "2      outbreak  0.017662\n",
            "3          case  0.014807\n",
            "4         novel  0.012517\n",
            "5           say  0.012240\n",
            "6           new  0.011796\n",
            "7         virus  0.009994\n",
            "8        people  0.009341\n",
            "9      hospital  0.008908\n",
            "10       health  0.008284\n",
            "11        death  0.007757\n",
            "12       spread  0.007376\n",
            "13      patient  0.006800\n",
            "14      confirm  0.006526\n",
            "15         city  0.005970\n",
            "16       report  0.005428\n",
            "17        fight  0.005180\n",
            "18          day  0.005033\n",
            "19      country  0.004991\n",
            "20        world  0.004861\n",
            "21        hubei  0.004841\n",
            "22     province  0.004739\n",
            "23        first  0.004635\n",
            "24      medical  0.004473\n",
            "25     epidemic  0.004472\n",
            "26     official  0.004120\n",
            "27         year  0.004089\n",
            "28         amid  0.003840\n",
            "29       infect  0.003747\n",
            "\n",
            "last:\n",
            "                  0         1\n",
            "587         journal  0.000312\n",
            "588           entry  0.000312\n",
            "589           water  0.000311\n",
            "590          plasma  0.000310\n",
            "591         current  0.000310\n",
            "592            loss  0.000310\n",
            "593          tariff  0.000309\n",
            "594        cambodia  0.000308\n",
            "595          recent  0.000308\n",
            "596      foundation  0.000308\n",
            "597           young  0.000307\n",
            "598            view  0.000307\n",
            "599            hurt  0.000307\n",
            "600           often  0.000306\n",
            "601           empty  0.000306\n",
            "602        previous  0.000306\n",
            "603          mobile  0.000306\n",
            "604            port  0.000305\n",
            "605         healthy  0.000305\n",
            "606        building  0.000304\n",
            "607          detail  0.000304\n",
            "608           joint  0.000303\n",
            "609            rush  0.000303\n",
            "610           liner  0.000303\n",
            "611          detect  0.000302\n",
            "612  transportation  0.000302\n",
            "613          canada  0.000302\n",
            "614          female  0.000301\n",
            "615         trigger  0.000301\n",
            "616         message  0.000301\n"
          ]
        }
      ],
      "source": [
        "print('China: ', len(threshold(pr_China,0.0003)))\n",
        "print()\n",
        "print('first:')\n",
        "print(threshold(pr_China,0.0003).iloc[:30])\n",
        "print()\n",
        "print('last:')\n",
        "print(threshold(pr_China,0.0003).iloc[len(threshold(pr_China,0.0003))-30:])\n",
        "print()\n",
        "print('USA: ', len(threshold(pr_USA,0.0003)))\n",
        "print()\n",
        "print('first:')\n",
        "print(threshold(pr_USA,0.0003).iloc[:30])\n",
        "print()\n",
        "print('last:')\n",
        "print(threshold(pr_USA,0.0003).iloc[len(threshold(pr_USA,0.0003))-30:])\n",
        "print()\n",
        "print('China&USA: ', len(threshold(pr_China_USA,0.0003)))\n",
        "print()\n",
        "print('first:')\n",
        "print(threshold(pr_China_USA,0.0003).iloc[:30])\n",
        "print()\n",
        "print('last:')\n",
        "print(threshold(pr_China_USA,0.0003).iloc[len(threshold(pr_China_USA,0.0003))-30:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqlZ2nZ9qKW"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "gVbC8VFA9pgw"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,1))   # ngram range can be changed to obtain measures regarding n grams instead of single words\n",
        "\n",
        "X_China = tfidf.fit_transform(cleaned_mostfreq_text_China).toarray()    # entry (i,j) if Tfidf measure of word_list[j] in document i\n",
        "word_list_China = tfidf.get_feature_names_out()\n",
        "\n",
        "X_USA = tfidf.fit_transform(cleaned_mostfreq_text_USA).toarray()\n",
        "word_list_USA = tfidf.get_feature_names_out()\n",
        "\n",
        "X_China_USA = tfidf.fit_transform(cleaned_mostfreq_text_China_USA).toarray()\n",
        "word_list_China_USA = tfidf.get_feature_names_out()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "8k8fcvKX-yah"
      },
      "outputs": [],
      "source": [
        "tfidf_df_China = pd.DataFrame(X_China,columns = word_list_China)\n",
        "\n",
        "tfidf_df_USA = pd.DataFrame(X_USA,columns = word_list_USA)\n",
        "\n",
        "tfidf_df_China_USA = pd.DataFrame(X_China_USA,columns = word_list_China_USA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "MUr9_93WCKsj"
      },
      "outputs": [],
      "source": [
        "tfidf_word_measure_China = np.mean(tfidf_df_China,axis = 0)\n",
        "tfidf_word_measure_China = tfidf_word_measure_China.sort_values(ascending = False)\n",
        "tfidf_word_measure_USA = np.mean(tfidf_df_USA,axis = 0)\n",
        "tfidf_word_measure_USA = tfidf_word_measure_USA.sort_values(ascending = False)\n",
        "tfidf_word_measure_China_USA = np.mean(tfidf_df_China_USA,axis = 0)\n",
        "tfidf_word_measure_China_USA = tfidf_word_measure_China_USA.sort_values(ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(tfidf_word_measure_China[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(tfidf_word_measure_USA[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(tfidf_word_measure_China_USA[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dknv21f3XimF",
        "outputId": "880e96e6-bc8f-414a-e3d4-7509caec4494"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "china          0.057393\n",
            "covid          0.032344\n",
            "case           0.029077\n",
            "novel          0.027626\n",
            "coronavirus    0.027115\n",
            "new            0.025339\n",
            "vaccine        0.022859\n",
            "hospital       0.022712\n",
            "say            0.020808\n",
            "outbreak       0.019470\n",
            "report         0.017295\n",
            "fight          0.016957\n",
            "patient        0.016629\n",
            "people         0.015936\n",
            "confirm        0.014500\n",
            "watch          0.014347\n",
            "health         0.014288\n",
            "country        0.014000\n",
            "day            0.013577\n",
            "live           0.013453\n",
            "province       0.012905\n",
            "first          0.012837\n",
            "city           0.012793\n",
            "death          0.012317\n",
            "year           0.012240\n",
            "world          0.011623\n",
            "epidemic       0.011616\n",
            "amid           0.011082\n",
            "medical        0.011039\n",
            "president      0.010959\n",
            "dtype: float64\n",
            "\n",
            "USA:\n",
            "coronavirus    0.049972\n",
            "covid          0.048809\n",
            "china          0.041930\n",
            "vaccine        0.031179\n",
            "case           0.028555\n",
            "say            0.027612\n",
            "new            0.026551\n",
            "report         0.017860\n",
            "death          0.017719\n",
            "outbreak       0.017477\n",
            "virus          0.016571\n",
            "test           0.016000\n",
            "spread         0.014537\n",
            "people         0.013405\n",
            "health         0.013065\n",
            "trump          0.012863\n",
            "first          0.012383\n",
            "late           0.012190\n",
            "president      0.011938\n",
            "day            0.011102\n",
            "million        0.010978\n",
            "state          0.010971\n",
            "rise           0.010859\n",
            "country        0.010299\n",
            "world          0.009807\n",
            "positive       0.009792\n",
            "pandemic       0.009256\n",
            "infection      0.009202\n",
            "hit            0.008676\n",
            "confirm        0.008295\n",
            "dtype: float64\n",
            "\n",
            "China&USA:\n",
            "china          0.045033\n",
            "coronavirus    0.043029\n",
            "covid          0.042957\n",
            "case           0.027942\n",
            "vaccine        0.027795\n",
            "new            0.025423\n",
            "say            0.024511\n",
            "outbreak       0.017291\n",
            "report         0.017268\n",
            "death          0.015774\n",
            "virus          0.013796\n",
            "people         0.013548\n",
            "test           0.013425\n",
            "health         0.013033\n",
            "first          0.012092\n",
            "novel          0.011916\n",
            "hospital       0.011775\n",
            "spread         0.011651\n",
            "day            0.011465\n",
            "president      0.011090\n",
            "country        0.011073\n",
            "late           0.010992\n",
            "million        0.010481\n",
            "trump          0.010241\n",
            "confirm        0.010155\n",
            "world          0.009889\n",
            "state          0.009267\n",
            "infection      0.009075\n",
            "city           0.009014\n",
            "patient        0.008786\n",
            "dtype: float64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reduced graph"
      ],
      "metadata": {
        "id": "4nkLA8k0LB7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# less frequent words:\n",
        "less_important_words_China = [key for key in list(threshold_reverse(pr_China,0.0003)[0])]\n",
        "\n",
        "less_important_words_USA = [key for key in list(threshold_reverse(pr_USA,0.0003)[0])]\n",
        "\n",
        "less_important_words_China_USA = [key for key in list(threshold_reverse(pr_China_USA,0.0003)[0])]"
      ],
      "metadata": {
        "id": "6ndwCL9oLpDA"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# discard less frequent words\n",
        "cleaned_mostimp_text_China = cleaned_mostfreq_text_China.copy()\n",
        "for txt in range(len(cleaned_mostimp_text_China)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostimp_text_China))\n",
        "  for word in less_important_words_China:\n",
        "    if word in cleaned_mostimp_text_China[txt].split():\n",
        "      cleaned_mostimp_text_China[txt] = cleaned_mostimp_text_China[txt].replace(word, '')\n",
        "      cleaned_mostimp_text_China[txt] = \" \".join(cleaned_mostimp_text_China[txt].split())\n",
        "\n",
        "cleaned_mostimp_text_USA = cleaned_mostfreq_text_USA.copy()\n",
        "for txt in range(len(cleaned_mostimp_text_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostimp_text_USA))\n",
        "  for word in less_important_words_USA:\n",
        "    if word in cleaned_mostimp_text_USA[txt].split():\n",
        "      cleaned_mostimp_text_USA[txt] = cleaned_mostimp_text_USA[txt].replace(word, '')\n",
        "      cleaned_mostimp_text_USA[txt] = \" \".join(cleaned_mostimp_text_USA[txt].split())\n",
        "\n",
        "cleaned_mostimp_text_China_USA = cleaned_mostfreq_text_China_USA.copy()\n",
        "for txt in range(len(cleaned_mostimp_text_China_USA)):\n",
        "  if txt % 1000 == 0:\n",
        "    print(txt, '/',len(cleaned_mostimp_text_China_USA))\n",
        "  for word in less_important_words_China_USA:\n",
        "    if word in cleaned_mostimp_text_China_USA[txt].split():\n",
        "      cleaned_mostimp_text_China_USA[txt] = cleaned_mostimp_text_China_USA[txt].replace(word, '')\n",
        "      cleaned_mostimp_text_China_USA[txt] = \" \".join(cleaned_mostimp_text_China_USA[txt].split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a737273-8260-456b-b291-f30f255affe3",
        "id": "D8L4ETxALpDB"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 4843\n",
            "1000 / 4843\n",
            "2000 / 4843\n",
            "3000 / 4843\n",
            "4000 / 4843\n",
            "0 / 7519\n",
            "1000 / 7519\n",
            "2000 / 7519\n",
            "3000 / 7519\n",
            "4000 / 7519\n",
            "5000 / 7519\n",
            "6000 / 7519\n",
            "7000 / 7519\n",
            "0 / 12362\n",
            "1000 / 12362\n",
            "2000 / 12362\n",
            "3000 / 12362\n",
            "4000 / 12362\n",
            "5000 / 12362\n",
            "6000 / 12362\n",
            "7000 / 12362\n",
            "8000 / 12362\n",
            "9000 / 12362\n",
            "10000 / 12362\n",
            "11000 / 12362\n",
            "12000 / 12362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_mostimp_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_mostimp_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_mostimp_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce63554-e53e-409a-8255-34961929980c",
        "id": "X2ObCNtJLpDB"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  663\n",
            "USA:  611\n",
            "China&USA:  652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_China = pd.DataFrame.from_dict(freq_dict_China, orient='index').reset_index()\n",
        "df_China.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China.reset_index(inplace=True)\n",
        "df_China.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_USA = pd.DataFrame.from_dict(freq_dict_USA, orient='index').reset_index()\n",
        "df_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_USA.reset_index(inplace=True)\n",
        "df_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_China_USA = pd.DataFrame.from_dict(freq_dict_China_USA, orient='index').reset_index()\n",
        "df_China_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China_USA.reset_index(inplace=True)\n",
        "df_China_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "print('China')\n",
        "print(df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA')\n",
        "print(df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA')\n",
        "print(df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10d32d2-9daf-47b6-df79-b32081870aa1",
        "id": "DfrViAqzNnb9"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "           Word  Count\n",
            "0         china   3008\n",
            "1         novel   1385\n",
            "2      hospital    969\n",
            "3   coronavirus    949\n",
            "4          case    863\n",
            "5      outbreak    846\n",
            "6           new    657\n",
            "7       patient    651\n",
            "8         fight    583\n",
            "9           say    521\n",
            "10      confirm    454\n",
            "11       people    426\n",
            "12     epidemic    403\n",
            "13      medical    378\n",
            "14        hubei    369\n",
            "15     province    355\n",
            "16       battle    311\n",
            "17      support    310\n",
            "18       report    299\n",
            "19         live    294\n",
            "20       health    294\n",
            "21         city    293\n",
            "22         amid    289\n",
            "23        death    288\n",
            "24          day    273\n",
            "25      control    266\n",
            "26        virus    263\n",
            "27         year    233\n",
            "28         take    223\n",
            "29       effort    213\n",
            "\n",
            "USA\n",
            "           Word  Count\n",
            "0   coronavirus   5134\n",
            "1         china   3946\n",
            "2      outbreak   1404\n",
            "3         virus   1133\n",
            "4          case   1110\n",
            "5           say    933\n",
            "6           new    906\n",
            "7        spread    858\n",
            "8         death    734\n",
            "9        people    624\n",
            "10       health    603\n",
            "11         late    459\n",
            "12       report    414\n",
            "13       cruise    414\n",
            "14        first    413\n",
            "15         ship    408\n",
            "16         city    394\n",
            "17      confirm    392\n",
            "18         toll    374\n",
            "19        world    370\n",
            "20   quarantine    369\n",
            "21        japan    366\n",
            "22        state    362\n",
            "23       travel    348\n",
            "24      country    347\n",
            "25         fear    329\n",
            "26       flight    329\n",
            "27          hit    321\n",
            "28         test    310\n",
            "29         rise    304\n",
            "\n",
            "China&USA\n",
            "           Word  Count\n",
            "0         china   6954\n",
            "1   coronavirus   6083\n",
            "2      outbreak   2250\n",
            "3          case   1973\n",
            "4           new   1563\n",
            "5         novel   1539\n",
            "6           say   1454\n",
            "7         virus   1396\n",
            "8      hospital   1177\n",
            "9        people   1050\n",
            "10        death   1022\n",
            "11       spread   1013\n",
            "12       health    897\n",
            "13      patient    851\n",
            "14      confirm    846\n",
            "15       report    713\n",
            "16        fight    707\n",
            "17         city    687\n",
            "18         late    638\n",
            "19        first    600\n",
            "20        hubei    588\n",
            "21          day    568\n",
            "22     epidemic    565\n",
            "23        world    562\n",
            "24      country    558\n",
            "25         amid    533\n",
            "26     province    525\n",
            "27      medical    495\n",
            "28     official    484\n",
            "29       infect    462\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_China = freq_dict_China.keys()  \n",
        "keys_USA = freq_dict_USA.keys()  \n",
        "keys_China_USA = freq_dict_China_USA.keys()  "
      ],
      "metadata": {
        "id": "Q52SCkkKNncF"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_China, network_df_China = create_network(cleaned_mostimp_text_China)\n",
        "network_USA, network_df_USA = create_network(cleaned_mostimp_text_USA)\n",
        "network_China_USA, network_df_China_USA = create_network(cleaned_mostimp_text_China_USA)"
      ],
      "metadata": {
        "id": "bPn3tfv7NncF"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(network_df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(network_df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(network_df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6772a419-9fd9-423f-ca28-2295767b13f7",
        "id": "MVggBElVNncF"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "                         weight\n",
            "(china, novel)             1018\n",
            "(china, outbreak)           644\n",
            "(china, coronavirus)        619\n",
            "(confirm, case)             571\n",
            "(patient, hospital)         560\n",
            "(china, case)               527\n",
            "(china, fight)              496\n",
            "(novel, coronavirus)        483\n",
            "(new, case)                 479\n",
            "(china, new)                426\n",
            "(china, say)                422\n",
            "(novel, outbreak)           394\n",
            "(case, report)              368\n",
            "(case, death)               357\n",
            "(epidemic, china)           346\n",
            "(china, hospital)           334\n",
            "(china, support)            333\n",
            "(china, confirm)            311\n",
            "(new, confirm)              296\n",
            "(china, people)             278\n",
            "(china, province)           272\n",
            "(china, battle)             266\n",
            "(china, hubei)              258\n",
            "(new, death)                245\n",
            "(report, new)               245\n",
            "(makeshift, hospital)       233\n",
            "(fight, novel)              233\n",
            "(china, effort)             232\n",
            "(novel, hospital)           231\n",
            "(coronavirus, outbreak)     227\n",
            "\n",
            "USA:\n",
            "                         weight\n",
            "(china, coronavirus)       2552\n",
            "(coronavirus, outbreak)    1118\n",
            "(china, outbreak)           937\n",
            "(coronavirus, case)         923\n",
            "(china, virus)              713\n",
            "(spread, coronavirus)       682\n",
            "(new, coronavirus)          642\n",
            "(china, case)               627\n",
            "(say, coronavirus)          605\n",
            "(china, death)              596\n",
            "(china, new)                561\n",
            "(coronavirus, death)        534\n",
            "(china, say)                521\n",
            "(china, spread)             504\n",
            "(coronavirus, health)       467\n",
            "(people, coronavirus)       463\n",
            "(china, people)             426\n",
            "(cruise, ship)              375\n",
            "(china, late)               367\n",
            "(confirm, case)             364\n",
            "(coronavirus, late)         363\n",
            "(china, health)             360\n",
            "(death, toll)               358\n",
            "(travel, china)             349\n",
            "(china, toll)               327\n",
            "(coronavirus, confirm)      326\n",
            "(coronavirus, cruise)       325\n",
            "(coronavirus, first)        318\n",
            "(report, coronavirus)       310\n",
            "(coronavirus, ship)         306\n",
            "\n",
            "China&USA:\n",
            "                         weight\n",
            "(china, coronavirus)       3171\n",
            "(china, outbreak)          1581\n",
            "(coronavirus, outbreak)    1345\n",
            "(china, case)              1154\n",
            "(coronavirus, case)        1146\n",
            "(china, novel)             1103\n",
            "(china, new)                987\n",
            "(china, say)                943\n",
            "(confirm, case)             935\n",
            "(china, virus)              914\n",
            "(coronavirus, new)          806\n",
            "(china, death)              804\n",
            "(new, case)                 783\n",
            "(spread, coronavirus)       724\n",
            "(china, people)             704\n",
            "(say, coronavirus)          682\n",
            "(novel, coronavirus)        638\n",
            "(case, death)               625\n",
            "(china, spread)             603\n",
            "(patient, hospital)         594\n",
            "(coronavirus, death)        585\n",
            "(health, china)             555\n",
            "(china, fight)              552\n",
            "(case, report)              543\n",
            "(coronavirus, people)       526\n",
            "(health, coronavirus)       513\n",
            "(china, confirm)            511\n",
            "(china, report)             491\n",
            "(china, late)               489\n",
            "(new, death)                457\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph(network):\n",
        "  up_weighted = []\n",
        "  for edge in network:\n",
        "      #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
        "      up_weighted.append((edge[0],edge[1],network[edge]))\n",
        "      \n",
        "  #print(network)\n",
        "  #print(up_weighted[0:10])\n",
        "  G = nx.Graph()\n",
        "  G.add_weighted_edges_from(up_weighted)\n",
        "  return G"
      ],
      "metadata": {
        "id": "fmQTFAb3NncF"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_China = get_graph(network_China)\n",
        "G_USA = get_graph(network_USA)\n",
        "G_China_USA = get_graph(network_China_USA)"
      ],
      "metadata": {
        "id": "zNrH5boVNncF"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print('Nodes: ',len(G_China.nodes()))\n",
        "print('Edges: ',len(G_China.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China))\n",
        "print()\n",
        "print('USA:')\n",
        "print('Nodes: ',len(G_USA.nodes()))\n",
        "print('Edges: ',len(G_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_USA))\n",
        "print()\n",
        "print('China&USA:')\n",
        "print('Nodes: ',len(G_China_USA.nodes()))\n",
        "print('Edges: ',len(G_China_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China_USA))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c7dec5-79bb-48cd-ab11-811490daffe2",
        "id": "kx5041hFNncF"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "Nodes:  663\n",
            "Edges:  46516\n",
            "Is connected:  True\n",
            "\n",
            "USA:\n",
            "Nodes:  611\n",
            "Edges:  40691\n",
            "Is connected:  True\n",
            "\n",
            "China&USA:\n",
            "Nodes:  652\n",
            "Edges:  63889\n",
            "Is connected:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save edge list"
      ],
      "metadata": {
        "id": "p4SvWlzSfzFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = './edgelist_China_1_700.csv'\n",
        "nx.write_weighted_edgelist(G_China, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_China_1_700.csv\n",
        "files.download(\"edgelist_China_1_700.csv\")\n",
        "\n",
        "filename = './edgelist_USA_1_700.csv'\n",
        "nx.write_weighted_edgelist(G_USA, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_USA_1_700.csv\n",
        "files.download(\"edgelist_USA_1_700.csv\")\n",
        "\n",
        "filename = './edgelist_China_USA_1_700.csv'\n",
        "nx.write_weighted_edgelist(G_China_USA, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_China_USA_1_700.csv\n",
        "files.download(\"edgelist_China_USA_1_700.csv\")"
      ],
      "metadata": {
        "id": "zgmmRGRCrM9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0929517e-af65-4e84-9939-be1d419ba6b4"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_04d3e31a-b57d-47c2-b42f-170bb3c1fa41\", \"edgelist_China_1_700.csv\", 741941)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c61aa16f-571e-4739-85ee-49f18724d89f\", \"edgelist_USA_1_700.csv\", 637663)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b5f7e79b-71c4-4d9f-b4b3-644284c55106\", \"edgelist_China_USA_1_700.csv\", 1005079)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create Node List\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nWHLO7AhdwzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nodes(freq_dict, name):\n",
        "  word_nodes = pd.DataFrame.from_dict(freq_dict,orient=\"index\")\n",
        "  word_nodes.reset_index(inplace=True)\n",
        "  word_nodes[\"Label\"] = word_nodes[\"index\"]\n",
        "  word_nodes.rename(columns={\"index\":\"Id\",0:\"delete\"},inplace=True)\n",
        "  word_nodes = word_nodes.drop(columns=['delete'])\n",
        "  nodelist = pd.DataFrame()\n",
        "  nodelist = nodelist.append(word_nodes, ignore_index=True)\n",
        "\n",
        "  nodelist = nodelist.to_csv(\"nodelist_\"+name+\".csv\",index=False)\n",
        "  files.download(\"nodelist_\"+name+\".csv\")\n",
        "  return nodelist, word_nodes"
      ],
      "metadata": {
        "id": "v2GYb2BQFzET"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodelist_China, word_nodes_China = nodes(freq_dict_China,\"China_1_700\")\n",
        "nodelist_USA, word_nodes_USA = nodes(freq_dict_USA,\"USA_1_700\")\n",
        "nodelist_China_USA, word_nodes_China_USA = nodes(freq_dict_China_USA,\"China_1_USA_700\")\n",
        "\n",
        "print('China:')\n",
        "print(word_nodes_China.head())\n",
        "print()\n",
        "print('USA:')\n",
        "print(word_nodes_USA.head())\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(word_nodes_China_USA.head())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "EmQrEHV0F1QY",
        "outputId": "e0a38312-9cb3-4c49-9713-e96898851063"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_466d0454-c5d8-481c-8de0-b1052b3c308e\", \"nodelist_China_1_700.csv\", 9291)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d91f9db8-22c4-4e75-bb03-a07548865deb\", \"nodelist_USA_1_700.csv\", 8361)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_749de18c-3943-4ce8-885c-bef478165da9\", \"nodelist_China_1_USA_700.csv\", 8837)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "            Id        Label\n",
            "0        china        china\n",
            "1        novel        novel\n",
            "2     hospital     hospital\n",
            "3  coronavirus  coronavirus\n",
            "4         case         case\n",
            "\n",
            "USA:\n",
            "            Id        Label\n",
            "0  coronavirus  coronavirus\n",
            "1        china        china\n",
            "2     outbreak     outbreak\n",
            "3        virus        virus\n",
            "4         case         case\n",
            "\n",
            "China&USA:\n",
            "            Id        Label\n",
            "0        china        china\n",
            "1  coronavirus  coronavirus\n",
            "2     outbreak     outbreak\n",
            "3         case         case\n",
            "4          new          new\n",
            "\n"
          ]
        }
      ]
    }
  ]
}